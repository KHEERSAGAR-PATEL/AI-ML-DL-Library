{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFMp5aGAgIg2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PatchEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, patch_size=16, embedding_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = layers.Conv2D(filters=embedding_dim, kernel_size=patch_size, strides=patch_size, padding='valid')\n",
        "        self.flatten = layers.Reshape((-1, embedding_dim))  # (B, num_patches, dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.proj(x)            # [B, H/P, W/P, D]\n",
        "        x = self.flatten(x)         # [B, N, D]\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttentionBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim=768, num_heads=12, attn_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim // num_heads, dropout=attn_dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = self.norm(x)\n",
        "        attn_output = self.mha(x_norm, x_norm)\n",
        "        return attn_output\n",
        "\n",
        "class MLPBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim=768, mlp_size=3072, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.mlp = keras.Sequential([\n",
        "            layers.Dense(mlp_size, activation='gelu'),\n",
        "            layers.Dropout(dropout),\n",
        "            layers.Dense(embedding_dim),\n",
        "            layers.Dropout(dropout)\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.mlp(x)\n",
        "\n",
        "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embedding_dim=768, num_heads=12, mlp_size=3072, mlp_dropout=0.1, attn_dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttentionBlock(embedding_dim, num_heads, attn_dropout)\n",
        "        self.mlp = MLPBlock(embedding_dim, mlp_size, mlp_dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.attn(x) + x\n",
        "        x = self.mlp(x) + x\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_channels=3,\n",
        "                 num_transformer_layers=12,\n",
        "                 embedding_dim=768,\n",
        "                 mlp_size=3072,\n",
        "                 num_heads=12,\n",
        "                 attn_dropout=0.0,\n",
        "                 mlp_dropout=0.1,\n",
        "                 embedding_dropout=0.1,\n",
        "                 num_classes=1000):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0, \"Image size must be divisible by patch size.\"\n",
        "\n",
        "        self.num_patches = (img_size * img_size) // (patch_size ** 2)\n",
        "        self.patch_embedding = PatchEmbedding(patch_size=patch_size, embedding_dim=embedding_dim)\n",
        "\n",
        "        self.class_token = self.add_weight(\"cls\", shape=[1, 1, embedding_dim], initializer=\"random_normal\", trainable=True)\n",
        "        self.position_embedding = self.add_weight(\"pos_embed\", shape=[1, self.num_patches + 1, embedding_dim],\n",
        "                                                  initializer=\"random_normal\", trainable=True)\n",
        "        self.embedding_dropout = layers.Dropout(embedding_dropout)\n",
        "\n",
        "        self.encoder_blocks = [TransformerEncoderBlock(embedding_dim, num_heads, mlp_size, mlp_dropout, attn_dropout)\n",
        "                               for _ in range(num_transformer_layers)]\n",
        "\n",
        "        self.norm = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.head = layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, x):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        x = self.patch_embedding(x)\n",
        "        cls_tokens = tf.broadcast_to(self.class_token, [batch_size, 1, self.class_token.shape[-1]])\n",
        "        x = tf.concat([cls_tokens, x], axis=1)\n",
        "        x = x + self.position_embedding\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        for blk in self.encoder_blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])  # Use only [CLS] token\n"
      ]
    }
  ]
}