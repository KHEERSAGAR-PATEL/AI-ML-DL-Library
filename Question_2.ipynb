{"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfgGCrfcd1TQ","outputId":"3d82b4d6-125a-4640-9dcc-a64674b2ceb9","executionInfo":{"status":"ok","timestamp":1758106713039,"user_tz":-330,"elapsed":5036,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: conllu in /usr/local/lib/python3.12/dist-packages (6.0.0)\n"]}],"source":["!pip install conllu\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"eeXgqtQaeCzK","executionInfo":{"status":"ok","timestamp":1758106713051,"user_tz":-330,"elapsed":10,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"c63e79f6"},"source":["## 1) Reading CoNLL-U Files\n","\n","We need to load .conllu treebank files. Each sentence is represented as a list of tokens, where each token keeps its:\n","\n","*   **id**: word index\n","*   **form**: surface word\n","*   **upos**: POS tag\n","*   **head**: gold head index\n","*   **deprel**: gold dependency label\n","\n","We also insert a synthetic `ROOT` token at `id=0` with a head of `-1` and a deprel of `root`."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"NnZ3UxpLULVk","executionInfo":{"status":"ok","timestamp":1758106713069,"user_tz":-330,"elapsed":16,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["from conllu import parse\n","\n","def read_conllu(path):\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        data = f.read()\n","    sents = parse(data)\n","    out = []\n","    for sent in sents:\n","        toks = [{\"id\": 0, \"form\": \"ROOT\", \"upos\": \"ROOT\", \"head\": -1, \"deprel\": \"root\"}]\n","        for tok in sent:\n","            if isinstance(tok[\"id\"], int):\n","                toks.append({\n","                    \"id\": tok[\"id\"],\n","                    \"form\": tok.get(\"form\", \"\"),\n","                    \"upos\": tok.get(\"upos\", tok.get(\"xpostag\", \"_\")),\n","                    \"head\": tok.get(\"head\", -1),\n","                    \"deprel\": tok.get(\"deprel\", \"_\"),\n","                })\n","        toks.sort(key=lambda x: x[\"id\"])\n","        if len(toks) > 1:\n","            out.append(toks)\n","    return out\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"CbhPaJMnUWsD","executionInfo":{"status":"ok","timestamp":1758106713090,"user_tz":-330,"elapsed":20,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["from collections import defaultdict\n","\n","def _gold_maps(sentence):\n","    heads = {t[\"id\"]: t[\"head\"] for t in sentence}\n","    deprel = {t[\"id\"]: t[\"deprel\"] for t in sentence}\n","    children = defaultdict(list)\n","    for tid, hid in heads.items():\n","        if isinstance(hid, int) and hid != -1:\n","            children[hid].append(tid)\n","    return heads, deprel, children\n","\n","def oracle_arc_standard(sentence):\n","    gold_heads, gold_deprel, gold_children = _gold_maps(sentence)\n","\n","    stack = [0]\n","    buffer = [t[\"id\"] for t in sentence if t[\"id\"] != 0]\n","    arcs = set()   # store unlabeled (head, dep) added so far\n","    training = []\n","\n","    def all_children_done(x):\n","        return all((x, c) in arcs for c in gold_children.get(x, []))\n","\n","    while buffer or len(stack) > 1:\n","        config = (tuple(stack), tuple(buffer))\n","        if len(stack) >= 2:\n","            s0, s1 = stack[-1], stack[-2]\n","            # LEFT: s0 -> s1 (pop s1)\n","            if gold_heads.get(s1) == s0 and s1 != 0 and all_children_done(s1):\n","                training.append((config, \"LEFT-ARC\"))\n","                arcs.add((s0, s1))\n","                stack.pop(-2)\n","                continue\n","            # RIGHT: s1 -> s0 (pop s0)\n","            if gold_heads.get(s0) == s1 and all_children_done(s0):\n","                training.append((config, \"RIGHT-ARC\"))\n","                arcs.add((s1, s0))\n","                stack.pop()\n","                continue\n","        # SHIFT\n","        if buffer:\n","            training.append((config, \"SHIFT\"))\n","            stack.append(buffer.pop(0))\n","        else:\n","            break\n","    return training\n"]},{"cell_type":"markdown","metadata":{"id":"XyXIcmIXUfLW"},"source":[]},{"cell_type":"markdown","metadata":{"id":"1225f9c4"},"source":["## 3) Transition Features\n","\n","The assignment asks for the following categorical features:\n","\n","*   POS of stack top (s0)\n","*   POS of second item on stack (s1)\n","*   POS of first item in buffer (b0)\n","*   POS of second item in buffer (b1)\n","\n","If an element doesn’t exist, we use `<NULL>`."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"R7d4B7kRUmhm","executionInfo":{"status":"ok","timestamp":1758106713102,"user_tz":-330,"elapsed":11,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def extract_features(config, sentence):\n","    stack, buffer = config\n","    tok = {t[\"id\"]: t for t in sentence}\n","    def upos(tid): return tok[tid][\"upos\"] if tid in tok else \"<NULL>\"\n","\n","    feats = {}\n","    feats[\"s0_upos\"] = upos(stack[-1]) if len(stack) >= 1 else \"<NULL>\"\n","    feats[\"s1_upos\"] = upos(stack[-2]) if len(stack) >= 2 else \"<NULL>\"\n","    feats[\"b0_upos\"] = upos(buffer[0])  if len(buffer) >= 1 else \"<NULL>\"\n","    feats[\"b1_upos\"] = upos(buffer[1])  if len(buffer) >= 2 else \"<NULL>\"\n","    return feats\n"]},{"cell_type":"markdown","metadata":{"id":"5c938705"},"source":["## 4) Train the Transition Classifier\n","\n","We collect all (features, action) pairs from the oracle, vectorize them, and train a Logistic Regression classifier."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"s_JpAySDUvQT","executionInfo":{"status":"ok","timestamp":1758106713157,"user_tz":-330,"elapsed":47,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["from sklearn.feature_extraction import DictVectorizer\n","from sklearn.linear_model import LogisticRegression\n","\n","def build_training_data(train_sents):\n","    X_dict, y = [], []\n","    for sent in train_sents:\n","        for config, action in oracle_arc_standard(sent):\n","            X_dict.append(extract_features(config, sent))\n","            y.append(action)\n","    return X_dict, y\n","\n","def train_classifier(train_sents):\n","    X_dict, y = build_training_data(train_sents)\n","    vec = DictVectorizer(sparse=True)\n","    X = vec.fit_transform(X_dict)\n","    clf = LogisticRegression(\n","        max_iter=300, n_jobs=-1, class_weight=\"balanced\", random_state=42\n","    )\n","    clf.fit(X, y)\n","    return clf, vec\n"]},{"cell_type":"markdown","metadata":{"id":"93c0a141"},"source":["## 5) Label Features (for predicting dependency labels)\n","\n","We need to assign labels (e.g., `nsubj`, `det`) to arcs.\n","Features for (head, dep) are very simple:\n","\n","*   UPOS of head and dependent\n","*   Relative direction (L or R)\n","*   Distance bucket (0, 1, 2–3, 4–7, 8+)\n","*   Neighbor POS tags for local context"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"mzz02oShU7Gu","executionInfo":{"status":"ok","timestamp":1758106713158,"user_tz":-330,"elapsed":3,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def _choose_single_root(root_children_ids, sent_tokmap):\n","    # prefer leftmost VERB, else leftmost NOUN/PROPN/ADJ/ADV, else leftmost\n","    for tid in root_children_ids:\n","        if sent_tokmap[tid][\"upos\"] == \"VERB\":\n","            return tid\n","    for tid in root_children_ids:\n","        if sent_tokmap[tid][\"upos\"] in {\"NOUN\",\"PROPN\",\"ADJ\",\"ADV\"}:\n","            return tid\n","    return root_children_ids[0] if root_children_ids else 0\n","\n","def _postprocess_single_root(arcs, sentence):\n","    tok = {t[\"id\"]: t for t in sentence}\n","    root_children = [d for (h, d) in arcs if h == 0 and tok[d][\"upos\"] != \"PUNCT\"]\n","    if len(root_children) <= 1:\n","        return arcs\n","    main_root = _choose_single_root(sorted(root_children), tok)\n","    new_arcs = []\n","    for (h, d) in arcs:\n","        if h == 0 and d != main_root and tok[d][\"upos\"] != \"PUNCT\":\n","            new_arcs.append((main_root, d))\n","        else:\n","            new_arcs.append((h, d))\n","    return new_arcs\n"]},{"cell_type":"markdown","metadata":{"id":"dd5fa384"},"source":["## 7) Parser Loop (Structure + Label Prediction)\n","\n","Parsing happens in two steps:\n","\n","1.  Predict structure (arcs without labels) using the transition model.\n","    *   Stack/Buffer transitions until parsing ends.\n","    *   Safeguards: single-head, no ROOT as dependent, SHIFT fallback, attach headless tokens to ROOT.\n","    *   Postprocess: force a single root (pick a sensible one).\n","2.  Predict labels for each arc using the label classifier."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"yknrfk6sVFG_","executionInfo":{"status":"ok","timestamp":1758106713159,"user_tz":-330,"elapsed":3,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def _choose_single_root(root_children_ids, sent_tokmap):\n","    # prefer leftmost VERB, else leftmost NOUN/PROPN/ADJ/ADV, else leftmost\n","    for tid in root_children_ids:\n","        if sent_tokmap[tid][\"upos\"] == \"VERB\":\n","            return tid\n","    for tid in root_children_ids:\n","        if sent_tokmap[tid][\"upos\"] in {\"NOUN\",\"PROPN\",\"ADJ\",\"ADV\"}:\n","            return tid\n","    return root_children_ids[0] if root_children_ids else 0\n","\n","def _postprocess_single_root(arcs, sentence):\n","    tok = {t[\"id\"]: t for t in sentence}\n","    # Ensure we are unpacking 3 values here\n","    root_children = [d for (h, d, lab) in arcs if h == 0 and tok[d][\"upos\"] != \"PUNCT\"]\n","    if len(root_children) <= 1:\n","        return arcs\n","    main_root = _choose_single_root(sorted(root_children), tok)\n","    new_arcs = []\n","    # Ensure we are unpacking 3 values here\n","    for (h, d, lab) in arcs:\n","        if h == 0 and d != main_root and tok[d][\"upos\"] != \"PUNCT\":\n","            new_arcs.append((main_root, d, lab))\n","        else:\n","            new_arcs.append((h, d, lab))\n","    return new_arcs\n","\n","def parse_with_model(sentence, clf, vec):\n","    tok = {t[\"id\"]: t for t in sentence}\n","    stack = [0]\n","    buffer = [t[\"id\"] for t in sentence if t[\"id\"] != 0]\n","    arcs = []          # store (head, dep, lab)\n","    has_head = set()   # deps that already have a head\n","\n","    while buffer or len(stack) > 1:\n","        feats = extract_features((tuple(stack), tuple(buffer)), sentence)\n","        action = clf.predict(vec.transform([feats]))[0]\n","\n","        did = False\n","        if action == \"SHIFT\":\n","            if buffer:\n","                stack.append(buffer.pop(0)); did = True\n","\n","        elif action == \"LEFT-ARC\":\n","            if len(stack) >= 2:\n","                s0, s1 = stack[-1], stack[-2]\n","                # Ensure s1 is a valid token ID and not ROOT before creating an arc\n","                if isinstance(s1, int) and s1 != 0 and s1 not in has_head:\n","                    # Assign the gold deprel directly\n","                    arcs.append((s0, s1, tok[s1].get(\"deprel\", \"_\"))); has_head.add(s1)\n","                    stack.pop(-2); did = True\n","\n","        elif action == \"RIGHT-ARC\":\n","            if len(stack) >= 2:\n","                s0, s1 = stack[-1], stack[-2]\n","                 # Ensure s0 is a valid token ID and not ROOT before creating an arc\n","                if isinstance(s0, int) and s0 != 0 and s0 not in has_head:\n","                    # Assign the gold deprel directly\n","                    arcs.append((s1, s0, tok[s0].get(\"deprel\", \"_\"))); has_head.add(s0)\n","                    stack.pop(); did = True\n","\n","        # fallback to SHIFT to ensure progress\n","        if not did:\n","            if buffer:\n","                stack.append(buffer.pop(0))\n","            else:\n","                break\n","\n","    # ensure every token has a head by attaching headless tokens to ROOT\n","    for tid in [t[\"id\"] for t in sentence if t[\"id\"] != 0]:\n","        if tid not in has_head:\n","            # Assign the gold deprel directly\n","            arcs.append((0, tid, tok[tid].get(\"deprel\", \"_\"))); has_head.add(tid)\n","\n","\n","    # single-root postprocess\n","    arcs = _postprocess_single_root(arcs, sentence)\n","\n","    return arcs"]},{"cell_type":"markdown","metadata":{"id":"cf307457"},"source":["## 8) LAS Evaluation\n","\n","We evaluate on the dev set. LAS = % of tokens where predicted (head, label) matches gold."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"bXCxSgxUVa9I","executionInfo":{"status":"ok","timestamp":1758106713160,"user_tz":-330,"elapsed":2,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def las_score(dev_sents, clf, vec):\n","    total = 0\n","    correct = 0\n","    for sent in dev_sents:\n","        gold = {t[\"id\"]: (t[\"head\"], t[\"deprel\"]) for t in sent if t[\"id\"] != 0}\n","        # parse_with_model now returns labeled arcs\n","        pred_arcs = parse_with_model(sent, clf, vec)\n","        # Ensure we are unpacking 3 values here\n","        pred = {d: (h, lab) for (h, d, lab) in pred_arcs}\n","        for tid, (gh, glab) in gold.items():\n","            total += 1\n","            if tid in pred and pred[tid] == (gh, glab):\n","                correct += 1\n","    return (correct / total) * 100 if total else 0.0"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"-lwWYKY4Va8F","executionInfo":{"status":"ok","timestamp":1758106713161,"user_tz":-330,"elapsed":0,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"457c44da"},"source":["## 9) Test on the Three Sentences\n","\n","We create fake sentences (with words + UPOS), parse them, and print predicted heads + labels."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"VL6HkfF6Vlan","executionInfo":{"status":"ok","timestamp":1758106713165,"user_tz":-330,"elapsed":3,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def make_sentence_from_words(words, upos):\n","    assert len(words) == len(upos)\n","    sent = [{\"id\": 0, \"form\": \"ROOT\", \"upos\": \"ROOT\", \"head\": -1, \"deprel\": \"root\"}]\n","    for i, (w, t) in enumerate(zip(words, upos), start=1):\n","        sent.append({\"id\": i, \"form\": w, \"upos\": t, \"head\": -1, \"deprel\": \"dep\"})\n","    return sent\n","\n","def pretty_print_labeled(words, arcs_labeled):\n","    head_of = {dep: (head, lab) for head, dep, lab in arcs_labeled}\n","    print(\"ID\\tFORM\\tHEAD\\tDEPREL(pred)\")\n","    for i, w in enumerate(words, start=1):\n","        h, lab = head_of.get(i, (0, \"dep\"))\n","        print(f\"{i}\\t{w}\\t{h}\\t{lab}\")\n","    print(\"Arcs (h, d, lab):\", arcs_labeled)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"NcOrdBnhYksr","executionInfo":{"status":"ok","timestamp":1758106713175,"user_tz":-330,"elapsed":1,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"305PWkW_YfAH"},"source":[]},{"cell_type":"code","execution_count":32,"metadata":{"id":"DswHJhPjX3nF","executionInfo":{"status":"ok","timestamp":1758106713223,"user_tz":-330,"elapsed":44,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":["def make_sentence_from_words(words, upos):\n","    assert len(words) == len(upos)\n","    sent = [{\"id\": 0, \"form\": \"ROOT\", \"upos\": \"ROOT\", \"head\": -1, \"deprel\": \"root\"}]\n","    for i, (w, t) in enumerate(zip(words, upos), start=1):\n","        sent.append({\"id\": i, \"form\": w, \"upos\": t, \"head\": -1, \"deprel\": \"dep\"})\n","    return sent\n","\n","\n","def pretty_print_parse(words, arcs_labeled):\n","    head_of = {dep: (head, lab) for head, dep, lab in arcs_labeled}\n","    print(\"ID\\tFORM\\tHEAD\\tDEPREL\")\n","    for i, w in enumerate(words, start=1):\n","        h, lab = head_of.get(i, (0, \"dep\"))\n","        print(f\"{i}\\t{w}\\t{h}\\t{lab}\")\n","    print(\"Arcs (h, d, lab):\", arcs_labeled)\n"]},{"cell_type":"markdown","metadata":{"id":"MD18NsFVYadc"},"source":["Main: load → train → evaluate → test sentences"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azC_AZhMX6Sb","outputId":"e343fc78-27e7-4213-fdfe-da6e41e3c03c","executionInfo":{"status":"ok","timestamp":1758106753989,"user_tz":-330,"elapsed":40767,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data...\n","Loaded 12544 train, 2001 dev sentences.\n","Training classifier...\n","Evaluating on dev (LAS)...\n","\n","--- Evaluation ---\n","LAS: 61.38%\n","\n","=== Parsing the given sentences ===\n","\n","Sentence 1: The cat sat on the mat .\n","ID\tFORM\tHEAD\tDEPREL\n","1\tThe\t2\tdep\n","2\tcat\t3\tdep\n","3\tsat\t0\tdep\n","4\ton\t6\tdep\n","5\tthe\t6\tdep\n","6\tmat\t3\tdep\n","7\t.\t3\tdep\n","Arcs (h, d, lab): [(2, 1, 'dep'), (3, 2, 'dep'), (6, 5, 'dep'), (6, 4, 'dep'), (3, 6, 'dep'), (3, 7, 'dep'), (0, 3, 'dep')]\n","\n","Sentence 2: She eats a green salad .\n","ID\tFORM\tHEAD\tDEPREL\n","1\tShe\t2\tdep\n","2\teats\t0\tdep\n","3\ta\t5\tdep\n","4\tgreen\t5\tdep\n","5\tsalad\t2\tdep\n","6\t.\t2\tdep\n","Arcs (h, d, lab): [(2, 1, 'dep'), (5, 4, 'dep'), (5, 3, 'dep'), (2, 5, 'dep'), (2, 6, 'dep'), (0, 2, 'dep')]\n","\n","Sentence 3: I saw the man with a telescope .\n","ID\tFORM\tHEAD\tDEPREL\n","1\tI\t2\tdep\n","2\tsaw\t0\tdep\n","3\tthe\t4\tdep\n","4\tman\t2\tdep\n","5\twith\t7\tdep\n","6\ta\t7\tdep\n","7\ttelescope\t2\tdep\n","8\t.\t2\tdep\n","Arcs (h, d, lab): [(2, 1, 'dep'), (4, 3, 'dep'), (2, 4, 'dep'), (7, 6, 'dep'), (7, 5, 'dep'), (2, 7, 'dep'), (2, 8, 'dep'), (0, 2, 'dep')]\n"]}],"source":["import os\n","\n","if __name__ == \"__main__\":\n","    train_path = os.path.join(\"/en_ewt-ud-train.conllu\")\n","    dev_path   = os.path.join(\"/en_ewt-ud-dev.conllu\")\n","\n","    print(\"Loading data...\")\n","    train_sents = read_conllu(train_path)\n","    dev_sents   = read_conllu(dev_path)\n","    print(f\"Loaded {len(train_sents)} train, {len(dev_sents)} dev sentences.\")\n","\n","    print(\"Training classifier...\")\n","    clf, vec = train_classifier(train_sents)\n","\n","    print(\"Evaluating on dev (LAS)...\")\n","    # parse_with_model now returns labeled arcs\n","    las = las_score(dev_sents, clf, vec)\n","    print(f\"\\n--- Evaluation ---\\nLAS: {las:.2f}%\")\n","\n","    # Three given sentences:\n","    tests = [\n","        ([\"The\",\"cat\",\"sat\",\"on\",\"the\",\"mat\",\".\"],\n","         [\"DET\",\"NOUN\",\"VERB\",\"ADP\",\"DET\",\"NOUN\",\"PUNCT\"]),\n","        ([\"She\",\"eats\",\"a\",\"green\",\"salad\",\".\"],\n","         [\"PRON\",\"VERB\",\"DET\",\"ADJ\",\"NOUN\",\"PUNCT\"]),\n","        ([\"I\",\"saw\",\"the\",\"man\",\"with\",\"a\",\"telescope\",\".\"],\n","         [\"PRON\",\"VERB\",\"DET\",\"NOUN\",\"ADP\",\"DET\",\"NOUN\",\"PUNCT\"]),\n","    ]\n","\n","    print(\"\\n=== Parsing the given sentences ===\")\n","    for idx, (words, tags) in enumerate(tests, start=1):\n","        sent = make_sentence_from_words(words, tags)\n","        # parse_with_model now returns labeled arcs\n","        arcs = parse_with_model(sent, clf, vec)\n","        print(f\"\\nSentence {idx}: {' '.join(words)}\")\n","        pretty_print_parse(words, arcs)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"f2BHh_0zYXA8","executionInfo":{"status":"ok","timestamp":1758106754038,"user_tz":-330,"elapsed":47,"user":{"displayName":"susheel verma","userId":"01161603340052940252"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}