{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest\n",
        " A Random Forest classifier that combines multiple decision trees for better accuracy.\n",
        "    Each tree is trained on a random subset of data and features to improve model diversity.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_estimators: int\n",
        "        How many trees to create in the forest.\n",
        "    max_features: int\n",
        "        The maximum number of features each tree can use to make its splits.\n",
        "    min_samples_split: int\n",
        "        Minimum samples required to split an internal node.\n",
        "    min_gain: float\n",
        "        Minimum decrease in impurity needed to continue splitting.\n",
        "    max_depth: int\n",
        "        Maximum depth of each tree to prevent overfitting."
      ],
      "metadata": {
        "id": "4_E6UHHkV9eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function\n",
        "import numpy as np\n",
        "import math\n",
        "import progressbar\n",
        "\n",
        "# Import helper modules and functions\n",
        "from mlfromscratch.utils import divide_on_feature, train_test_split, get_random_subsets, normalize\n",
        "from mlfromscratch.utils import accuracy_score, calculate_entropy\n",
        "from mlfromscratch.unsupervised_learning import PCA\n",
        "from mlfromscratch.supervised_learning import ClassificationTree\n",
        "from mlfromscratch.utils.misc import bar_widgets\n",
        "from mlfromscratch.utils import Plot\n",
        "\n",
        "\n",
        "class RandomForest:\n",
        "\n",
        "    def __init__(self, n_estimators=100, max_features=None, min_samples_split=2,\n",
        "                 min_gain=0, max_depth=float(\"inf\")):\n",
        "        # Store parameters to control tree and forest behavior\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_gain = min_gain\n",
        "        self.max_depth = max_depth\n",
        "        self.progressbar = progressbar.ProgressBar(widgets=bar_widgets)\n",
        "\n",
        "        # Set up an empty list to hold our trees\n",
        "        self.trees = []\n",
        "        # Initialize each tree with specified parameters\n",
        "        for _ in range(n_estimators):\n",
        "            tree = ClassificationTree(\n",
        "                min_samples_split=self.min_samples_split,\n",
        "                min_impurity=min_gain,\n",
        "                max_depth=self.max_depth\n",
        "            )\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Get the total number of features in the dataset\n",
        "        n_features = np.shape(X)[1]\n",
        "\n",
        "        # If max_features is not defined, we’ll set it to sqrt(n_features)\n",
        "        if not self.max_features:\n",
        "            self.max_features = int(math.sqrt(n_features))\n",
        "\n",
        "        # Create a list of random subsets of the data (one for each tree)\n",
        "        subsets = get_random_subsets(X, y, self.n_estimators)\n",
        "\n",
        "        # Train each tree on its own random subset of data\n",
        "        for i in self.progressbar(range(self.n_estimators)):\n",
        "            X_subset, y_subset = subsets[i]\n",
        "\n",
        "            # Randomly select which features to use for this tree\n",
        "            feature_indices = np.random.choice(range(n_features), size=self.max_features, replace=True)\n",
        "\n",
        "            # Save these feature indices for later (each tree will have its own set)\n",
        "            self.trees[i].feature_indices = feature_indices\n",
        "\n",
        "            # Reduce our subset to only the chosen features\n",
        "            X_subset = X_subset[:, feature_indices]\n",
        "\n",
        "            # Now, train the tree on this modified subset\n",
        "            self.trees[i].fit(X_subset, y_subset)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # We’ll store each tree's predictions in this array\n",
        "        y_preds = np.empty((X.shape[0], len(self.trees)))\n",
        "\n",
        "        # Loop over each tree and get its predictions\n",
        "        for i, tree in enumerate(self.trees):\n",
        "            # Use only the features this tree was trained on\n",
        "            feature_indices = tree.feature_indices\n",
        "            # Get the tree's predictions on the given samples\n",
        "            predictions = tree.predict(X[:, feature_indices])\n",
        "            # Store these predictions in our array\n",
        "            y_preds[:, i] = predictions\n",
        "\n",
        "        # Now combine each sample's predictions from all trees\n",
        "        y_pred = [np.bincount(sample_preds.astype(int)).argmax() for sample_preds in y_preds]\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "ocZteVGvWAl1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}