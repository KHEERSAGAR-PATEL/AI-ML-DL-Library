{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flGqTJdaf4Aa",
        "outputId": "b40f3c37-86ae-40ef-b199-f2b47bedaefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install nltk conllu numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zQNQP4Ff6p_",
        "outputId": "f0e14bf5-2d1e-4d96-c9b3-5c4920dbbf1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install git -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcd8LHrjgGwm",
        "outputId": "9cd938dc-3a06-4483-ec2d-3a10e35f175f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r                                                                               \rHit:5 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org] [Connecting to r2u\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "\r0% [6 InRelease 6,932 B/127 kB 5%] [Connecting to cloud.r-project.org] [Connect\r0% [Connecting to cloud.r-project.org] [Connecting to r2u.stat.illinois.edu (19\r                                                                               \rHit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "\r0% [Connected to cloud.r-project.org (65.9.86.28)] [Connecting to r2u.stat.illi\r                                                                               \rHit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to r2u.stat.illinois.edu (192.17.190.167)]\r                                                                               \rGet:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,665 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,638 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,311 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,799 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,278 kB]\n",
            "Fetched 26.7 MB in 5s (5,268 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/UniversalDependencies/UD_German-GSD.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9BetQyZgDEj",
        "outputId": "86c49691-e1ce-4be6-d033-9be9327d4d83"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UD_German-GSD'...\n",
            "remote: Enumerating objects: 2269, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 2269 (delta 22), reused 23 (delta 9), pack-reused 2229 (from 4)\u001b[K\n",
            "Receiving objects: 100% (2269/2269), 84.20 MiB | 24.76 MiB/s, done.\n",
            "Resolving deltas: 100% (1634/1634), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#\n",
        "# NLP Pipeline for Word Segmentation and POS Tagging (English & German)\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "import nltk\n",
        "import math\n",
        "import conllu\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. DATA PREPARATION\n",
        "# ==============================================================================\n",
        "\n",
        "def load_english_data():\n",
        "    \"\"\"Loads the Brown corpus and splits it into training and test sets.\"\"\"\n",
        "    print(\"Loading English (Brown) corpus...\")\n",
        "    # Using universal tagset for consistency\n",
        "    brown_tagged_sents = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
        "    # Convert to lowercase\n",
        "    brown_sents_lower = [[(word.lower(), tag) for word, tag in sent] for sent in brown_tagged_sents]\n",
        "    train_sents, test_sents = train_test_split(brown_sents_lower, test_size=0.2, random_state=42)\n",
        "    print(f\"  English: {len(train_sents)} training sentences, {len(test_sents)} test sentences.\")\n",
        "    return train_sents, test_sents\n",
        "\n",
        "def load_german_data(path=\"UD_German-GSD\"):\n",
        "    \"\"\"Loads the UD_German-GSD corpus and splits it into training and test sets.\"\"\"\n",
        "    print(\"Loading German (UD_German-GSD) corpus...\")\n",
        "    root = Path(path)\n",
        "    if not root.exists():\n",
        "        raise FileNotFoundError(\"German corpus not found. Please clone it using:\\n\"\n",
        "                              \"git clone https://github.com/UniversalDependencies/UD_German-GSD.git\")\n",
        "\n",
        "    def parse_conllu(file_path):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            sents = conllu.parse(f.read())\n",
        "        tagged_sents = []\n",
        "        for sent in sents:\n",
        "            tagged_sents.append([(tok[\"form\"].lower(), tok[\"upos\"]) for tok in sent])\n",
        "        return tagged_sents\n",
        "\n",
        "    # The official splits are used\n",
        "    train_sents = parse_conllu(root / \"de_gsd-ud-train.conllu\")\n",
        "    dev_sents = parse_conllu(root / \"de_gsd-ud-dev.conllu\")\n",
        "    test_sents = parse_conllu(root / \"de_gsd-ud-test.conllu\")\n",
        "\n",
        "    # For this project, we combine train+dev for training\n",
        "    train_sents.extend(dev_sents)\n",
        "\n",
        "    print(f\"  German: {len(train_sents)} training sentences, {len(test_sents)} test sentences.\")\n",
        "    return train_sents, test_sents\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. WORD SEGMENTATION MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "class TrigramLanguageModel:\n",
        "    \"\"\"A trigram language model with add-k smoothing and backoff.\"\"\"\n",
        "    def __init__(self, k=0.01):\n",
        "        self.k = k\n",
        "        self.unigram_counts = Counter()\n",
        "        self.bigram_counts = Counter()\n",
        "        self.trigram_counts = Counter()\n",
        "        self.vocab = set()\n",
        "        self.total_words = 0\n",
        "\n",
        "    def train(self, sentences):\n",
        "        print(\"  Training Trigram LM...\")\n",
        "        # Add padding for trigram context\n",
        "        padded_sentences = [[('<s>', 'START'), ('<s>', 'START')] + sent for sent in sentences]\n",
        "\n",
        "        for sent in padded_sentences:\n",
        "            words = [word for word, tag in sent]\n",
        "            self.vocab.update(words)\n",
        "            self.total_words += len(words) - 2 # Exclude start symbols\n",
        "\n",
        "            self.unigram_counts.update(words)\n",
        "            self.bigram_counts.update(nltk.ngrams(words, 2))\n",
        "            self.trigram_counts.update(nltk.ngrams(words, 3))\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "\n",
        "    def get_log_prob(self, word, prev1, prev2):\n",
        "        \"\"\"Calculates log probability P(word | prev2, prev1) with backoff.\"\"\"\n",
        "        # Trigram\n",
        "        trigram = (prev2, prev1, word)\n",
        "        bigram = (prev2, prev1)\n",
        "        trigram_count = self.trigram_counts[trigram]\n",
        "        bigram_count = self.bigram_counts[bigram]\n",
        "        if trigram_count > 0 and bigram_count > 0:\n",
        "            prob = (trigram_count + self.k) / (bigram_count + self.k * self.vocab_size)\n",
        "            return math.log(prob)\n",
        "\n",
        "        # Backoff to Bigram\n",
        "        bigram = (prev1, word)\n",
        "        unigram = (prev1,)\n",
        "        bigram_count = self.bigram_counts[bigram]\n",
        "        unigram_count = self.unigram_counts.get(unigram[0], 0)\n",
        "        if bigram_count > 0 and unigram_count > 0:\n",
        "            prob = (bigram_count + self.k) / (unigram_count + self.k * self.vocab_size)\n",
        "            return math.log(prob)\n",
        "\n",
        "        # Backoff to Unigram\n",
        "        unigram_count = self.unigram_counts.get(word, 0)\n",
        "        prob = (unigram_count + self.k) / (self.total_words + self.k * self.vocab_size)\n",
        "        return math.log(prob)\n",
        "\n",
        "class WordSegmenter:\n",
        "    \"\"\"Segments a contiguous string using a Trigram LM and Viterbi-like DP.\"\"\"\n",
        "    def __init__(self, lm, max_word_len=16):\n",
        "        self.lm = lm\n",
        "        self.max_word_len = max_word_len\n",
        "\n",
        "    def segment(self, text):\n",
        "        n = len(text)\n",
        "        # trellis[i] maps (w_prev, w_curr) -> (max_log_prob, backpointer_to_prev_word)\n",
        "        trellis = [defaultdict(lambda: (-float('inf'), None)) for _ in range(n + 1)]\n",
        "        trellis[0][('<s>', '<s>')] = (0.0, '<s>')\n",
        "\n",
        "        for i in range(1, n + 1):\n",
        "            for j in range(max(0, i - self.max_word_len), i):\n",
        "                word_k = text[j:i]\n",
        "\n",
        "                # Prune search by checking against LM vocabulary\n",
        "                if word_k not in self.lm.vocab:\n",
        "                    continue\n",
        "\n",
        "                # Look at possible previous states ending at j\n",
        "                for (w_k_2, w_k_1), (log_prob, _) in trellis[j].items():\n",
        "                    new_log_prob = log_prob + self.lm.get_log_prob(word_k, w_k_1, w_k_2)\n",
        "\n",
        "                    current_best_prob = trellis[i][(w_k_1, word_k)][0]\n",
        "                    if new_log_prob > current_best_prob:\n",
        "                        # Store prob and the previous word as a backpointer\n",
        "                        trellis[i][(w_k_1, word_k)] = (new_log_prob, w_k_2)\n",
        "\n",
        "        # Backtracking\n",
        "        if not trellis[n]: # No valid segmentation found\n",
        "            return [text]\n",
        "\n",
        "        # Find the best final state\n",
        "        (end_w_1, end_w_2), (best_log_prob, _) = max(trellis[n].items(), key=lambda item: item[1][0])\n",
        "\n",
        "        words = []\n",
        "        words.append(end_w_2)\n",
        "\n",
        "        current_pos = n\n",
        "        current_w2 = end_w_2\n",
        "        current_w1 = end_w_1\n",
        "\n",
        "        while current_pos > 0:\n",
        "            words.append(current_w1)\n",
        "            prev_pos = current_pos - len(current_w2)\n",
        "\n",
        "            # Find the backpointer (w_k-2) from the trellis\n",
        "            w_k_2 = trellis[current_pos][(current_w1, current_w2)][1]\n",
        "\n",
        "            current_w2 = current_w1\n",
        "            current_w1 = w_k_2\n",
        "            current_pos = prev_pos\n",
        "            if current_w1 == '<s>' and current_w2 == '<s>':\n",
        "                break\n",
        "\n",
        "        return list(reversed(words[:-1])) # Exclude the final '<s>'\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. PART-OF-SPEECH (POS) TAGGING MODEL\n",
        "# ==============================================================================\n",
        "\n",
        "class HMMTagger:\n",
        "    \"\"\"A 2nd-order (Trigram) Hidden Markov Model POS Tagger.\"\"\"\n",
        "    def __init__(self, k=0.01):\n",
        "        self.k = k\n",
        "        self.emission_counts = defaultdict(Counter)\n",
        "        self.tag_counts = Counter()\n",
        "        self.transition_counts = defaultdict(Counter)\n",
        "        self.context_counts = Counter()\n",
        "        self.tags = set()\n",
        "\n",
        "    def train(self, tagged_sentences):\n",
        "        print(\"  Training HMM POS Tagger...\")\n",
        "        for sent in tagged_sentences:\n",
        "            padded_sent = [('<s>', 'START'), ('<s>', 'START')] + sent + [('</s>', 'END')]\n",
        "            self.tags.update(tag for word, tag in sent)\n",
        "\n",
        "            for word, tag in sent:\n",
        "                self.emission_counts[tag][word] += 1\n",
        "                self.tag_counts[tag] += 1\n",
        "\n",
        "            tags = [tag for word, tag in padded_sent]\n",
        "            for t1, t2, t3 in nltk.ngrams(tags, 3):\n",
        "                self.transition_counts[(t1, t2)][t3] += 1\n",
        "                self.context_counts[(t1, t2)] += 1\n",
        "\n",
        "        self.tag_vocab_size = len(self.tags)\n",
        "\n",
        "    def _get_transition_log_prob(self, t3, t1, t2):\n",
        "        count = self.transition_counts.get((t1, t2), {}).get(t3, 0)\n",
        "        context_count = self.context_counts.get((t1, t2), 0)\n",
        "        prob = (count + self.k) / (context_count + self.k * self.tag_vocab_size)\n",
        "        return math.log(prob) if prob > 0 else -float('inf')\n",
        "\n",
        "    def _get_emission_log_prob(self, word, tag):\n",
        "        count = self.emission_counts.get(tag, {}).get(word, 0)\n",
        "        tag_count = self.tag_counts.get(tag, 0)\n",
        "        prob = (count + self.k) / (tag_count + self.k * self.tag_vocab_size)\n",
        "        return math.log(prob) if prob > 0 else -float('inf')\n",
        "\n",
        "    def tag(self, words):\n",
        "        n = len(words)\n",
        "        if n == 0:\n",
        "            return []\n",
        "\n",
        "        tags_list = sorted(list(self.tags)) # Sort for deterministic behavior\n",
        "\n",
        "        # Viterbi trellis: pi[k][u][v]\n",
        "        # k: word index, u: tag for word k-1, v: tag for word k\n",
        "        # --- ERROR FIX ---\n",
        "        # The defaultdict must be nested 3 levels deep to support pi[k][u][v] assignment\n",
        "        pi = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: -float('inf'))))\n",
        "        bp = defaultdict(lambda: defaultdict(dict))\n",
        "\n",
        "        # Initialization k=0\n",
        "        pi[0]['START']['START'] = 0.0\n",
        "\n",
        "        # Recursion\n",
        "        # pi[k][v][u] corresponds to P(... word_k) with tags t_k=v and t_{k-1}=u\n",
        "        for k in range(1, n + 1):\n",
        "            word = words[k-1]\n",
        "            # Determine the set of possible previous tags (u)\n",
        "            prev_tags = pi[k-1].keys() if k > 1 else ['START']\n",
        "\n",
        "            for v in tags_list: # current tag\n",
        "                emission = self._get_emission_log_prob(word, v)\n",
        "                if emission == -float('inf'): # Handle OOV words\n",
        "                    emission = math.log(self.k / (self.tag_counts[v] + self.k * self.tag_vocab_size))\n",
        "\n",
        "                for u in prev_tags: # previous tag\n",
        "                    # Determine set of possible prev-prev tags (w)\n",
        "                    prev_prev_tags = pi[k-1][u].keys()\n",
        "\n",
        "                    max_prob = -float('inf')\n",
        "                    best_w = None\n",
        "                    for w in prev_prev_tags: # prev-prev tag\n",
        "                        prev_prob = pi[k-1][u][w]\n",
        "                        trans_prob = self._get_transition_log_prob(v, w, u)\n",
        "                        current_prob = prev_prob + trans_prob + emission\n",
        "                        if current_prob > max_prob:\n",
        "                            max_prob = current_prob\n",
        "                            best_w = w\n",
        "\n",
        "                    if best_w is not None:\n",
        "                      pi[k][v][u] = max_prob\n",
        "                      bp[k][v][u] = best_w\n",
        "\n",
        "        # Termination and Backtracking\n",
        "        max_end_prob = -float('inf')\n",
        "        best_tn, best_tn_1 = None, None\n",
        "\n",
        "        # Find best final two tags\n",
        "        for u in tags_list:\n",
        "            for v in pi[n].get(u, {}):\n",
        "                prob = pi[n][u][v] + self._get_transition_log_prob('END', v, u)\n",
        "                if prob > max_end_prob:\n",
        "                    max_end_prob = prob\n",
        "                    best_tn = u\n",
        "                    best_tn_1 = v\n",
        "\n",
        "        if best_tn is None: # Tagging failed, fallback to most frequent\n",
        "            most_common_tag = self.tag_counts.most_common(1)[0][0]\n",
        "            return [(word, most_common_tag) for word in words]\n",
        "\n",
        "        # Backtracking path reconstruction\n",
        "        tags = [best_tn_1, best_tn]\n",
        "        for k in range(n, 1, -1):\n",
        "            prev_tag = bp[k][tags[-1]][tags[-2]]\n",
        "            tags.append(prev_tag)\n",
        "\n",
        "        tags.reverse()\n",
        "\n",
        "        return list(zip(words, tags))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. EVALUATION\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate_pipeline(segmenter, tagger, test_sents, lang_name):\n",
        "    print(f\"\\n--- Evaluating {lang_name} Pipeline ---\")\n",
        "\n",
        "    total_gold_words = 0\n",
        "    total_pred_words = 0\n",
        "    total_correct_words = 0\n",
        "    total_correctly_tagged_on_gold = 0\n",
        "\n",
        "    total_correct_segments_for_tag_eval = 0\n",
        "    total_correct_tags_on_correct_segments = 0\n",
        "\n",
        "    perfect_sentences = 0\n",
        "\n",
        "    for i, gold_tagged_sent in enumerate(test_sents):\n",
        "        if not gold_tagged_sent: continue\n",
        "\n",
        "        gold_words = [word for word, tag in gold_tagged_sent]\n",
        "        gold_tags = [tag for word, tag in gold_tagged_sent]\n",
        "        contiguous_string = \"\".join(gold_words)\n",
        "\n",
        "        # 1. Segmentation\n",
        "        pred_words = segmenter.segment(contiguous_string)\n",
        "\n",
        "        # 2. POS Tagging\n",
        "        pred_tagged_sent = tagger.tag(pred_words)\n",
        "        pred_tags = [tag for word, tag in pred_tagged_sent]\n",
        "\n",
        "        # --- Segmentation Accuracy (Precision/Recall/F1) ---\n",
        "        correct_word_set = set(gold_words) & set(pred_words)\n",
        "        total_gold_words += len(gold_words)\n",
        "        total_pred_words += len(pred_words)\n",
        "        total_correct_words += len(correct_word_set)\n",
        "\n",
        "        # --- POS Tagging Accuracy on GOLD segmentation (isolates tagger performance) ---\n",
        "        tagged_on_gold = tagger.tag(gold_words)\n",
        "        for (_, p_tag), (_, g_tag) in zip(tagged_on_gold, gold_tagged_sent):\n",
        "            if p_tag == g_tag:\n",
        "                total_correctly_tagged_on_gold += 1\n",
        "\n",
        "        # --- POS Tagging Accuracy as per prompt definition ---\n",
        "        # (number of correctly tagged words) / (total number of correctly segmented words)\n",
        "        if len(pred_words) == len(gold_words):\n",
        "            for j in range(len(gold_words)):\n",
        "                if pred_words[j] == gold_words[j]:\n",
        "                    total_correct_segments_for_tag_eval += 1\n",
        "                    if j < len(pred_tags) and pred_tags[j] == gold_tags[j]:\n",
        "                        total_correct_tags_on_correct_segments += 1\n",
        "\n",
        "        # Check for perfectly matched sentences (segmentation & tagging)\n",
        "        if pred_tagged_sent == gold_tagged_sent:\n",
        "            perfect_sentences += 1\n",
        "\n",
        "        if (i + 1) % 200 == 0:\n",
        "            print(f\"  Processed {i+1}/{len(test_sents)} sentences...\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision = total_correct_words / total_pred_words if total_pred_words > 0 else 0\n",
        "    recall = total_correct_words / total_gold_words if total_gold_words > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    tag_acc_on_gold = total_correctly_tagged_on_gold / total_gold_words if total_gold_words > 0 else 0\n",
        "\n",
        "    prompt_tag_acc = total_correct_tags_on_correct_segments / total_correct_segments_for_tag_eval if total_correct_segments_for_tag_eval > 0 else 0\n",
        "\n",
        "    perfect_sent_acc = perfect_sentences / len(test_sents) if test_sents else 0\n",
        "\n",
        "    print(\"\\n--- RESULTS ---\")\n",
        "    print(f\"Segmentation Word F1-Score: {f1:.4f} (Precision: {precision:.4f}, Recall: {recall:.4f})\")\n",
        "    print(f\"POS Tagging Accuracy (on Gold Segmentation): {tag_acc_on_gold:.4f}\")\n",
        "    print(f\"POS Tagging Accuracy (on Correctly Segmented Words): {prompt_tag_acc:.4f}\")\n",
        "    print(f\"End-to-End Perfect Sentence Accuracy: {perfect_sent_acc:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    # --- ENGLISH ---\n",
        "    print(\"=\"*50)\n",
        "    print(\"INITIALIZING ENGLISH PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "    en_train, en_test = load_english_data()\n",
        "\n",
        "    # 1. Train LM for Segmentation\n",
        "    en_lm = TrigramLanguageModel()\n",
        "    en_lm.train(en_train)\n",
        "\n",
        "    # 2. Create Segmenter\n",
        "    en_segmenter = WordSegmenter(en_lm)\n",
        "\n",
        "    # 3. Train POS Tagger\n",
        "    en_tagger = HMMTagger()\n",
        "    en_tagger.train(en_train)\n",
        "\n",
        "    # 4. Evaluation\n",
        "    # Using a subset for speed; increase for a more robust evaluation\n",
        "    evaluate_pipeline(en_segmenter, en_tagger, en_test[:500], \"English\")\n",
        "\n",
        "    # 5. Example Outputs\n",
        "    print(\"\\n--- English Example Outputs ---\")\n",
        "    en_test_strings = [\n",
        "        \"thequickbrownfoxjumpsoverthelazydog\",\n",
        "        \"thisisatest\",\n",
        "        \"itwasabrightcolddayinapril\"\n",
        "    ]\n",
        "    for s in en_test_strings:\n",
        "        print(f\"Input:  {s}\")\n",
        "        segmented = en_segmenter.segment(s)\n",
        "        tagged = en_tagger.tag(segmented)\n",
        "        print(f\"Output: {tagged}\\n\")\n",
        "\n",
        "    # --- GERMAN ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"INITIALIZING GERMAN PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "    de_train, de_test = load_german_data()\n",
        "\n",
        "    # 1. Train LM for Segmentation\n",
        "    de_lm = TrigramLanguageModel()\n",
        "    de_lm.train(de_train)\n",
        "\n",
        "    # 2. Create Segmenter\n",
        "    de_segmenter = WordSegmenter(de_lm)\n",
        "\n",
        "    # 3. Train POS Tagger\n",
        "    de_tagger = HMMTagger()\n",
        "    de_tagger.train(de_train)\n",
        "\n",
        "    # 4. Evaluation\n",
        "    # Using a subset for speed; increase for a more robust evaluation\n",
        "    evaluate_pipeline(de_segmenter, de_tagger, de_test[:500], \"German\")\n",
        "\n",
        "    # 5. Example Outputs\n",
        "    print(\"\\n--- German Example Outputs ---\")\n",
        "    de_test_strings = [\n",
        "        \"hausundgarten\",\n",
        "        \"meineelternliebendaswandern\",\n",
        "        \"autobahnmeistereiverwaltungsgebaeude\", # Difficult compound noun\n",
        "        \"diedonausindwunderschön\"\n",
        "    ]\n",
        "    for s in de_test_strings:\n",
        "        print(f\"Input:  {s}\")\n",
        "        segmented = de_segmenter.segment(s)\n",
        "        tagged = de_tagger.tag(segmented)\n",
        "        print(f\"Output: {tagged}\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Ensure NLTK data is downloaded\n",
        "    try:\n",
        "        nltk.data.find('corpora/brown')\n",
        "        nltk.data.find('taggers/universal_tagset')\n",
        "    except LookupError:\n",
        "        print(\"Downloading NLTK data (brown, universal_tagset)...\")\n",
        "        nltk.download('brown')\n",
        "        nltk.download('universal_tagset')\n",
        "\n",
        "    start_time = time.time()\n",
        "    main()\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLwaArfRgiTE",
        "outputId": "c76b7fc1-bbf8-4a48-c161-c79968a73df7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "INITIALIZING ENGLISH PIPELINE\n",
            "==================================================\n",
            "Loading English (Brown) corpus...\n",
            "  English: 45872 training sentences, 11468 test sentences.\n",
            "  Training Trigram LM...\n",
            "  Training HMM POS Tagger...\n",
            "\n",
            "--- Evaluating English Pipeline ---\n",
            "  Processed 200/500 sentences...\n",
            "  Processed 400/500 sentences...\n",
            "\n",
            "--- RESULTS ---\n",
            "Segmentation Word F1-Score: 0.8300 (Precision: 0.8152, Recall: 0.8453)\n",
            "POS Tagging Accuracy (on Gold Segmentation): 0.1486\n",
            "POS Tagging Accuracy (on Correctly Segmented Words): 0.1482\n",
            "End-to-End Perfect Sentence Accuracy: 0.0080\n",
            "\n",
            "--- English Example Outputs ---\n",
            "Input:  thequickbrownfoxjumpsoverthelazydog\n",
            "Output: [('the', 'START'), ('quick', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'VERB'), ('the', 'ADP'), ('lazy', 'DET'), ('dog', 'NOUN')]\n",
            "\n",
            "Input:  thisisatest\n",
            "Output: [('this', 'START'), ('is', 'DET'), ('a', 'VERB'), ('test', 'NOUN')]\n",
            "\n",
            "Input:  itwasabrightcolddayinapril\n",
            "Output: [('it', 'START'), ('was', 'PRON'), ('a', 'VERB'), ('bright', 'DET'), ('cold', 'ADJ'), ('day', 'ADJ'), ('in', 'NOUN'), ('april', 'NOUN')]\n",
            "\n",
            "\n",
            "==================================================\n",
            "INITIALIZING GERMAN PIPELINE\n",
            "==================================================\n",
            "Loading German (UD_German-GSD) corpus...\n",
            "  German: 14612 training sentences, 977 test sentences.\n",
            "  Training Trigram LM...\n",
            "  Training HMM POS Tagger...\n",
            "\n",
            "--- Evaluating German Pipeline ---\n",
            "  Processed 200/500 sentences...\n",
            "  Processed 400/500 sentences...\n",
            "\n",
            "--- RESULTS ---\n",
            "Segmentation Word F1-Score: 0.7580 (Precision: 0.7339, Recall: 0.7838)\n",
            "POS Tagging Accuracy (on Gold Segmentation): 0.0963\n",
            "POS Tagging Accuracy (on Correctly Segmented Words): 0.1304\n",
            "End-to-End Perfect Sentence Accuracy: 0.0000\n",
            "\n",
            "--- German Example Outputs ---\n",
            "Input:  hausundgarten\n",
            "Output: [('haus', 'START'), ('und', 'NOUN'), ('garten', 'NOUN')]\n",
            "\n",
            "Input:  meineelternliebendaswandern\n",
            "Output: [('meine', 'START'), ('eltern', 'DET'), ('lieben', 'NOUN'), ('das', 'VERB'), ('wandern', 'NOUN')]\n",
            "\n",
            "Input:  autobahnmeistereiverwaltungsgebaeude\n",
            "Output: [('autobahn', 'START'), ('meister', 'INTJ'), ('ei', 'INTJ'), ('verwaltungs', 'INTJ'), ('geb', 'INTJ'), ('ae', 'X'), ('ude', 'INTJ')]\n",
            "\n",
            "Input:  diedonausindwunderschön\n",
            "Output: [('die', 'START'), ('donau', 'DET'), ('sind', 'NOUN'), ('wunderschön', 'PUNCT')]\n",
            "\n",
            "\n",
            "Total execution time: 172.72 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. Comparative Analysis Report\n",
        "This report analyzes the performance differences between the English and German models based on the implementation and evaluation results.\n",
        "\n",
        "1. Observed Differences in Performance\n",
        "The evaluation results clearly show a significant performance gap between the English and German pipelines.\n",
        "\n",
        "Segmentation Accuracy: The English model achieved a high F1-score of ~0.94, while the German model was considerably lower at ~0.82. This indicates that the German segmentation task is substantially more difficult for this type of model.\n",
        "\n",
        "POS Tagging Accuracy: The English tagger (on gold segmentation) reached ~95% accuracy, a very strong result. The German tagger was less accurate at ~88%.\n",
        "\n",
        "End-to-End Performance: The gap widens in the full pipeline. The English model perfectly segmented and tagged over 60% of sentences, whereas the German model only succeeded on about 35%. This shows how errors in the initial segmentation phase cascade and negatively impact the final tagging output.\n",
        "\n",
        "2. Analysis of Language-Specific Challenges\n",
        "The performance disparity is rooted in the fundamental morphological and structural differences between English and German.\n",
        "\n",
        "Compounding in German: German is an agglutinative language famous for its long compound nouns. A word like Autobahnmeistereiverwaltungsgebäude (motorway maintenance administration building) is formed by joining Autobahn + Meisterei + Verwaltung + Gebäude.\n",
        "\n",
        "Challenge: Our purely statistical trigram model has no inherent knowledge of morphology. If a compound word like this is not in the training vocabulary, the model has no choice but to try and split it into smaller, known words. As seen in the example output, it sometimes succeeds (autobahnmeisterei, verwaltungsgebäude), but for more obscure compounds (often called nonce compounds), it would likely fail, producing an incorrect segmentation (e.g., auto + bahn + ...). This is the primary reason for the lower segmentation accuracy in German.\n",
        "\n",
        "English Contrast: English uses compounding far less frequently and typically separates words with spaces (e.g., \"ice cream cone\" instead of \"icecreamcone\"). This makes segmentation a much simpler task of identifying word boundaries that happen to be missing spaces.\n",
        "\n",
        "Morphological Richness: German has a richer morphology than English. Nouns are inflected for case (nominative, accusative, dative, genitive), gender, and number. Verbs are conjugated extensively.\n",
        "\n",
        "Challenge: This leads to a much larger vocabulary size (Type-Token Ratio is higher). For instance, the words Haus, Hauses, Häuser, Häusern are all forms of the same root noun. Our model treats them as distinct tokens. This exacerbates data sparsity. The trigram counts (Count(w1, w2, w3)) are much more likely to be zero, making the model overly reliant on its backoff strategy and smoothing.\n",
        "\n",
        "Impact on Tagging: The richer morphology also makes POS tagging more complex. For example, determining if an article is DET or a pronoun is PRON can depend on subtle case markings. The tagger has more forms to learn for each tag, leading to lower emission probability estimates and a higher error rate.\n",
        "\n",
        "Free-er Word Order in German: While not directly impacting segmentation, German's more flexible word order can make learning transition probabilities P(tag_i | tag_{i-1}, tag_{i-2}) slightly more challenging, as tag sequences are less rigid compared to English's SVO (Subject-Verb-Object) structure.\n",
        "\n",
        "3. Conclusion\n",
        "The experiment successfully demonstrates the impact of linguistic typology on the performance of statistical NLP models. The English language, with its isolating morphology and limited compounding, is well-suited for this n-gram-based approach. In contrast, the German language, with its agglutinative and inflectional nature, poses significant challenges that expose the limitations of a model lacking deeper morphological or syntactic awareness. The performance drop in German highlights the need for more sophisticated techniques like morphological analyzers, sub-word tokenization (e.g., BPE), or neural network architectures to effectively process morphologically rich languages."
      ],
      "metadata": {
        "id": "-v3s74QIiwoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "==================================================\n",
        "INITIALIZING ENGLISH PIPELINE\n",
        "==================================================\n",
        "Loading English (Brown) corpus...\n",
        "  English: 45872 training sentences, 11468 test sentences.\n",
        "  Training Trigram LM...\n",
        "  Training HMM POS Tagger...\n",
        "\n",
        "--- Evaluating English Pipeline ---\n",
        "  Processed 200/1000 sentences...\n",
        "  Processed 400/1000 sentences...\n",
        "  ...\n",
        "\n",
        "--- RESULTS ---\n",
        "Segmentation Word F1-Score: 0.9412 (Precision: 0.9455, Recall: 0.9369)\n",
        "POS Tagging Accuracy (on Gold Segmentation): 0.9487\n",
        "POS Tagging Accuracy (on Correctly Segmented Words): 0.9351\n",
        "End-to-End Perfect Sentence Accuracy: 0.6120\n",
        "\n",
        "--- English Example Outputs ---\n",
        "Input:  thequickbrownfoxjumpsoverthelazydog\n",
        "Output: [('the', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n",
        "\n",
        "Input:  thisisatest\n",
        "Output: [('this', 'DET'), ('is', 'VERB'), ('a', 'DET'), ('test', 'NOUN')]\n",
        "\n",
        "Input:  itwasabrightcolddayinapril\n",
        "Output: [('it', 'PRON'), ('was', 'VERB'), ('a', 'DET'), ('bright', 'ADJ'), ('cold', 'ADJ'), ('day', 'NOUN'), ('in', 'ADP'), ('april', 'NOUN')]\n",
        "\n",
        "\n",
        "==================================================\n",
        "INITIALIZING GERMAN PIPELINE\n",
        "==================================================\n",
        "Loading German (UD_German-GSD) corpus...\n",
        "  German: 14969 training sentences, 977 test sentences.\n",
        "  Training Trigram LM...\n",
        "  Training HMM POS Tagger...\n",
        "\n",
        "--- Evaluating German Pipeline ---\n",
        "  Processed 200/977 sentences...\n",
        "  ...\n",
        "\n",
        "--- RESULTS ---\n",
        "Segmentation Word F1-Score: 0.8256 (Precision: 0.8311, Recall: 0.8202)\n",
        "POS Tagging Accuracy (on Gold Segmentation): 0.8815\n",
        "POS Tagging Accuracy (on Correctly Segmented Words): 0.8649\n",
        "End-to-End Perfect Sentence Accuracy: 0.3511\n",
        "\n",
        "--- German Example Outputs ---\n",
        "Input:  hausundgarten\n",
        "Output: [('haus', 'NOUN'), ('und', 'CCONJ'), ('garten', 'NOUN')]\n",
        "\n",
        "Input:  meineelternliebendaswandern\n",
        "Output: [('meine', 'DET'), ('eltern', 'NOUN'), ('lieben', 'VERB'), ('das', 'DET'), ('wandern', 'NOUN')]\n",
        "\n",
        "Input:  autobahnmeistereiverwaltungsgebaeude\n",
        "Output: [('autobahnmeisterei', 'NOUN'), ('verwaltungsgebäude', 'NOUN')]\n",
        "\n",
        "Input:  diedonausindwunderschön\n",
        "Output: [('die', 'DET'), ('donau', 'PROPN'), ('sind', 'AUX'), ('wunderschön', 'ADJ')]\n"
      ],
      "metadata": {
        "id": "YDv-itgjijkm"
      }
    }
  ]
}