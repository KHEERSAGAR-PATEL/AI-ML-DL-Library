{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The code implements a single-layer Perceptron classifier from scratch. Perceptrons are simple neural networks used for binary classification, though they can also be adapted for multi-class tasks. This implementation includes a variety of configurable activation functions and loss functions, making it flexible for different applications.\n",
        "\n",
        "Overview of Key Components\n",
        "This code utilizes the Perceptron model, which follows a linear approach to map input features to target classes by learning a weight matrix and bias term. The model is trained using a gradient descent approach to iteratively adjust weights, reducing the error in predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "0dPjwwYmolnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import math\n",
        "import progressbar\n",
        "\n",
        "# Define commonly used activation functions and loss functions from scratch\n",
        "class Sigmoid:\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        sig = self.__call__(x)\n",
        "        return sig * (1 - sig)\n",
        "\n",
        "class SquareLoss:\n",
        "    def __call__(self, y_true, y_pred):\n",
        "        return 0.5 * np.power((y_true - y_pred), 2).mean()\n",
        "\n",
        "    def gradient(self, y_true, y_pred):\n",
        "        return -(y_true - y_pred)\n",
        "\n",
        "# Perceptron Model: A single-layer neural network classifier\n",
        "class Perceptron:\n",
        "    \"\"\" A simple one-layer Perceptron classifier.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    n_iterations : int\n",
        "        Number of training iterations\n",
        "    activation_function : object\n",
        "        Activation function class with __call__ and gradient methods\n",
        "    loss : object\n",
        "        Loss function class with __call__ and gradient methods\n",
        "    learning_rate : float\n",
        "        Rate at which weights are updated during training\n",
        "    \"\"\"\n",
        "    def __init__(self, n_iterations=1000, activation_function=Sigmoid, loss=SquareLoss, learning_rate=0.01):\n",
        "        self.n_iterations = n_iterations\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = loss()\n",
        "        self.activation_func = activation_function()\n",
        "        self.progressbar = progressbar.ProgressBar()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_outputs = y.shape[1] if len(y.shape) > 1 else 1\n",
        "\n",
        "        # Initialize weights in a small random range\n",
        "        limit = 1 / math.sqrt(n_features)\n",
        "        self.W = np.random.uniform(-limit, limit, (n_features, n_outputs))\n",
        "        self.w0 = np.zeros((1, n_outputs))\n",
        "\n",
        "        # Training loop\n",
        "        for i in self.progressbar(range(self.n_iterations)):\n",
        "            # Forward pass\n",
        "            linear_output = np.dot(X, self.W) + self.w0\n",
        "            y_pred = self.activation_func(linear_output)\n",
        "\n",
        "            # Compute the gradient of the error\n",
        "            error_gradient = self.loss.gradient(y, y_pred) * self.activation_func.gradient(linear_output)\n",
        "\n",
        "            # Calculate gradients for weights and bias\n",
        "            grad_wrt_w = np.dot(X.T, error_gradient)\n",
        "            grad_wrt_w0 = np.sum(error_gradient, axis=0, keepdims=True)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.W -= self.learning_rate * grad_wrt_w\n",
        "            self.w0 -= self.learning_rate * grad_wrt_w0\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Forward pass to compute output\n",
        "        linear_output = np.dot(X, self.W) + self.w0\n",
        "        y_pred = self.activation_func(linear_output)\n",
        "        return y_pred\n"
      ],
      "metadata": {
        "id": "zHJViBZjpJi8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Key Components\n",
        "Activation and Loss Functions:\n",
        "\n",
        "Sigmoid Activation: Defined with both __call__ (for forward pass) and gradient (for backpropagation) methods.\n",
        "Square Loss: Defined similarly with a method to compute the loss and another to compute the gradient.\n",
        "Perceptron Initialization:\n",
        "\n",
        "The __init__ method initializes the key parameters:\n",
        "n_iterations: Number of training steps.\n",
        "activation_function: The activation function used by the neurons.\n",
        "loss: The loss function to calculate prediction error.\n",
        "learning_rate: The speed at which the weights are updated.\n",
        "Training (fit method):\n",
        "\n",
        "The training loop iterates n_iterations times.\n",
        "In each iteration:\n",
        "Forward Pass: Computes the linear combination of inputs and weights, followed by the activation function to generate predictions.\n",
        "Error Calculation: Uses the loss function’s gradient to calculate error gradients for backpropagation.\n",
        "Weight Updates: Adjusts weights and biases by moving in the direction that reduces the loss.\n",
        "Prediction (predict method):\n",
        "\n",
        "Uses the trained model’s weights and biases to generate predictions for new data.\n",
        "Usage Example\n",
        "This code allows us to build and train a perceptron from scratch, which can be used for classification tasks."
      ],
      "metadata": {
        "id": "BZcKNIMCpOKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the Perceptron model\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample data\n",
        "    X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y_train = np.array([[0], [1], [1], [0]])  # XOR problem\n",
        "\n",
        "    # Initialize Perceptron model\n",
        "    perceptron = Perceptron(n_iterations=5000, activation_function=Sigmoid, loss=SquareLoss, learning_rate=0.1)\n",
        "\n",
        "    # Train model\n",
        "    perceptron.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = perceptron.predict(X_train)\n",
        "    print(\"Predictions:\\n\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjlSoAtbpUuv",
        "outputId": "a32c2856-6f03-47dc-de8f-6d8704555c18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[38;2;0;255;0m100%\u001b[39m \u001b[38;2;0;255;0m(5000 of 5000)\u001b[39m |####################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            " [[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iRFyVZhnokyi"
      }
    }
  ]
}