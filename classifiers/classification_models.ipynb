{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH4Z4rIUhGoG"
      },
      "source": [
        "Histogram-Based Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7OdERNUhGoJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prediction=None, feature_index=None, threshold=None, left=None, right=None):\n",
        "        self.prediction = prediction\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "\n",
        "class HistogramTree:\n",
        "    def __init__(self, max_depth=3, min_samples_split=2, n_bins=256):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_bins = n_bins\n",
        "        self.root = None\n",
        "        self.bins = None\n",
        "\n",
        "\n",
        "    def _bin_data(self, X):\n",
        "        \"\"\"Bin continuous data into discrete buckets for faster processing.\"\"\"\n",
        "        self.bins = np.linspace(np.min(X, axis=0), np.max(X, axis=0), self.n_bins)\n",
        "        X_binned = np.digitize(X, bins=self.bins) - 1  # Bin indices for each feature\n",
        "        return X_binned\n",
        "\n",
        "\n",
        "    def _calculate_leaf_value(self, gradients):\n",
        "        \"\"\"Calculate the prediction for a leaf node.\"\"\"\n",
        "        return np.mean(gradients)\n",
        "\n",
        "\n",
        "    def _best_split(self, X_binned, gradients):\n",
        "        \"\"\"Find the best split by iterating through all features and thresholds.\"\"\"\n",
        "        best_gain = -float(\"inf\")\n",
        "        best_split = {\"feature_index\": None, \"threshold\": None}\n",
        "\n",
        "\n",
        "        for feature_index in range(X_binned.shape[1]):\n",
        "            unique_thresholds = np.unique(X_binned[:, feature_index])\n",
        "            for threshold in unique_thresholds:\n",
        "                left_indices = X_binned[:, feature_index] <= threshold\n",
        "                right_indices = X_binned[:, feature_index] > threshold\n",
        "\n",
        "\n",
        "                if sum(left_indices) < self.min_samples_split or sum(right_indices) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                left_gradient = gradients[left_indices]\n",
        "                right_gradient = gradients[right_indices]\n",
        "\n",
        "\n",
        "                gain = self._calculate_gain(left_gradient, right_gradient)\n",
        "\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = threshold\n",
        "\n",
        "\n",
        "        return best_split if best_gain > 0 else None\n",
        "\n",
        "\n",
        "    def _calculate_gain(self, left_gradient, right_gradient):\n",
        "        \"\"\"Calculate gain based on reduction in variance.\"\"\"\n",
        "        var_total = np.var(np.concatenate([left_gradient, right_gradient]))\n",
        "        var_left = np.var(left_gradient)\n",
        "        var_right = np.var(right_gradient)\n",
        "\n",
        "\n",
        "        weight_left = len(left_gradient) / (len(left_gradient) + len(right_gradient))\n",
        "        weight_right = 1 - weight_left\n",
        "\n",
        "\n",
        "        gain = var_total - (weight_left * var_left + weight_right * var_right)\n",
        "        return gain\n",
        "\n",
        "\n",
        "    def _build_tree(self, X_binned, gradients, depth):\n",
        "        \"\"\"Recursively build the tree based on histogram binning and gain calculation.\"\"\"\n",
        "        if depth == self.max_depth or len(X_binned) < self.min_samples_split:\n",
        "            prediction = self._calculate_leaf_value(gradients)\n",
        "            return Node(prediction=prediction)\n",
        "\n",
        "\n",
        "        split = self._best_split(X_binned, gradients)\n",
        "        if split is None:\n",
        "            prediction = self._calculate_leaf_value(gradients)\n",
        "            return Node(prediction=prediction)\n",
        "\n",
        "\n",
        "        feature_index, threshold = split[\"feature_index\"], split[\"threshold\"]\n",
        "        left_indices = X_binned[:, feature_index] <= threshold\n",
        "        right_indices = ~left_indices\n",
        "\n",
        "\n",
        "        left_child = self._build_tree(X_binned[left_indices], gradients[left_indices], depth + 1)\n",
        "        right_child = self._build_tree(X_binned[right_indices], gradients[right_indices], depth + 1)\n",
        "\n",
        "\n",
        "        return Node(feature_index=feature_index, threshold=threshold, left=left_child, right=right_child)\n",
        "\n",
        "\n",
        "    def fit(self, X, gradients):\n",
        "        X_binned = self._bin_data(X)\n",
        "        self.root = self._build_tree(X_binned, gradients, 0)\n",
        "\n",
        "\n",
        "    def _predict_row(self, x):\n",
        "        node = self.root\n",
        "        while node.left or node.right:\n",
        "            if x[node.feature_index] <= node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.prediction\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_binned = np.digitize(X, bins=self.bins) - 1  # Use the trained binning\n",
        "        predictions = np.array([self._predict_row(row) for row in X_binned])\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class HistGradientBoostingClassifier:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2, n_bins=256):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_bins = n_bins\n",
        "        self.trees = []\n",
        "\n",
        "\n",
        "    def _initialize_predictions(self, y):\n",
        "        \"\"\"Initialize predictions to log-odds for binary classification.\"\"\"\n",
        "        p = np.clip(np.mean(y), 1e-5, 1 - 1e-5)\n",
        "        return np.log(p / (1 - p)) * np.ones(len(y))\n",
        "\n",
        "\n",
        "    def _compute_gradients(self, y, pred):\n",
        "        \"\"\"Compute the gradients for binary cross-entropy loss.\"\"\"\n",
        "        probs = 1 / (1 + np.exp(-pred))  # Sigmoid for probability\n",
        "        return y - probs\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        pred = self._initialize_predictions(y)\n",
        "        for _ in range(self.n_estimators):\n",
        "            gradients = self._compute_gradients(y, pred)\n",
        "            tree = HistogramTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, n_bins=self.n_bins)\n",
        "            tree.fit(X, gradients)\n",
        "            update = tree.predict(X)\n",
        "            pred += self.learning_rate * update\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Predict probabilities using the boosted ensemble of trees.\"\"\"\n",
        "        pred = self._initialize_predictions(np.zeros(X.shape[0]))\n",
        "        for tree in self.trees:\n",
        "            pred += self.learning_rate * tree.predict(X)\n",
        "        probs = 1 / (1 + np.exp(-pred))\n",
        "        return np.vstack((1 - probs, probs)).T\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict binary class based on the predicted probabilities.\"\"\"\n",
        "        probs = self.predict_proba(X)[:, 1]\n",
        "        return (probs >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# X = np.random.rand(100, 1)  # Sample feature data\n",
        "# y = np.random.randint(0, 2, 100)  # Sample binary targets\n",
        "# model = HistGradientBoostingClassifier(n_estimators=10, max_depth=3)\n",
        "# model.fit(X, y)\n",
        "# predictions = model.predict(X)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8tlW2BshGoK"
      },
      "source": [
        "AdaBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKh2bK4JhGoK",
        "outputId": "521b394a-6109-4a4b-a18b-2748fb0bdd8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [ 1.  1. -1.  1.  1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionStump:\n",
        "    \"\"\"\n",
        "    A simple decision stump (a one-level decision tree) used as the weak classifier in AdaBoost.\n",
        "    It finds the best feature and threshold to minimize classification error.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.polarity = 1  # Decides if feature values below or above the threshold belong to class 1\n",
        "        self.alpha = None  # Weight of the stump in final prediction\n",
        "\n",
        "\n",
        "    def fit(self, X, y, sample_weights):\n",
        "        \"\"\"\n",
        "        Train the stump using weighted training samples.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "        y (np.array): Target labels\n",
        "        sample_weights (np.array): Weights for each sample\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        min_error = float('inf')\n",
        "\n",
        "\n",
        "        # Loop over all features and find the best threshold for each\n",
        "        for feature_i in range(n_features):\n",
        "            feature_values = X[:, feature_i]\n",
        "            unique_values = np.unique(feature_values)\n",
        "\n",
        "\n",
        "            # Try each unique value in the feature as a threshold\n",
        "            for threshold in unique_values:\n",
        "                # Predict all samples as 1 or -1 based on the polarity\n",
        "                for polarity in [1, -1]:\n",
        "                    predictions = np.ones(n_samples)  # Initial predictions of 1\n",
        "                    predictions[feature_values < threshold] = -1 if polarity == 1 else 1\n",
        "\n",
        "\n",
        "                    # Calculate the error with the given polarity and threshold\n",
        "                    misclassified = predictions != y\n",
        "                    weighted_error = np.sum(sample_weights[misclassified])\n",
        "\n",
        "\n",
        "                    # Keep track of the best threshold and polarity\n",
        "                    if weighted_error < min_error:\n",
        "                        min_error = weighted_error\n",
        "                        self.polarity = polarity\n",
        "                        self.threshold = threshold\n",
        "                        self.feature_index = feature_i\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class for each sample in X based on the threshold and polarity.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        np.array: Predictions (-1 or 1)\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        predictions = np.ones(n_samples)\n",
        "        feature_values = X[:, self.feature_index]\n",
        "\n",
        "\n",
        "        # Apply polarity to determine which side of the threshold is class -1\n",
        "        if self.polarity == 1:\n",
        "            predictions[feature_values < self.threshold] = -1\n",
        "        else:\n",
        "            predictions[feature_values >= self.threshold] = -1\n",
        "\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AdaBoostClassifier:\n",
        "    \"\"\"\n",
        "    AdaBoost classifier that combines several decision stumps.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=50):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.stumps = []\n",
        "        self.alphas = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the AdaBoost model.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "        y (np.array): Target labels (assumed to be -1 and 1 for binary classification)\n",
        "        \"\"\"\n",
        "        n_samples, _ = X.shape\n",
        "        # Initialize weights for each sample\n",
        "        sample_weights = np.ones(n_samples) / n_samples\n",
        "\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Create a stump and train it\n",
        "            stump = DecisionStump()\n",
        "            stump.fit(X, y, sample_weights)\n",
        "            predictions = stump.predict(X)\n",
        "\n",
        "\n",
        "            # Calculate error and stump weight\n",
        "            error = np.sum(sample_weights[predictions != y])\n",
        "            if error > 0.5:\n",
        "                continue  # Skip if error is above 0.5; this stump is too weak\n",
        "            alpha = 0.5 * np.log((1.0 - error) / (error + 1e-10))  # Stump's weight in final classifier\n",
        "            stump.alpha = alpha  # Store weight for this stump\n",
        "\n",
        "\n",
        "            # Update weights for the samples\n",
        "            sample_weights *= np.exp(-alpha * y * predictions)\n",
        "            sample_weights /= np.sum(sample_weights)  # Normalize to sum to 1\n",
        "\n",
        "\n",
        "            # Save this stump and its weight\n",
        "            self.stumps.append(stump)\n",
        "            self.alphas.append(alpha)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for samples in X.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        np.array: Predicted labels (-1 or 1)\n",
        "        \"\"\"\n",
        "        # Weighted sum of predictions from each stump\n",
        "        stump_preds = np.array([stump.alpha * stump.predict(X) for stump in self.stumps])\n",
        "        y_pred = np.sum(stump_preds, axis=0)\n",
        "        return np.sign(y_pred)  # Return the sign as the predicted class (-1 or 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample dataset\n",
        "X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 5]])\n",
        "y = np.array([1, 1, -1, -1, 1])\n",
        "\n",
        "\n",
        "# Convert labels from (0, 1) to (-1, 1) if necessary\n",
        "y = np.where(y == 0, -1, y)\n",
        "\n",
        "\n",
        "# Initialize and train AdaBoost\n",
        "model = AdaBoostClassifier(n_estimators=10)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmmD_864hGoL"
      },
      "source": [
        "GradientBoostingCLassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e305mi44hGoL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class DecisionStump:\n",
        "    \"\"\"\n",
        "    A simple decision stump (one-level decision tree) used as the weak learner in Gradient Boosting.\n",
        "    It splits on a single feature with a threshold and predicts a constant value for each split.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.feature_index = None\n",
        "        self.threshold = None\n",
        "        self.left_value = None\n",
        "        self.right_value = None\n",
        "\n",
        "\n",
        "    def fit(self, X, residuals):\n",
        "        \"\"\"\n",
        "        Fit the stump on the data to minimize residual error.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "        residuals (np.array): Residuals (errors) of predictions from previous models\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        min_error = float('inf')\n",
        "\n",
        "\n",
        "        # Loop over each feature and threshold to find the best split\n",
        "        for feature_i in range(n_features):\n",
        "            feature_values = X[:, feature_i]\n",
        "            unique_values = np.unique(feature_values)\n",
        "\n",
        "\n",
        "            for threshold in unique_values:\n",
        "                left_indices = feature_values < threshold\n",
        "                right_indices = feature_values >= threshold\n",
        "\n",
        "\n",
        "                # Calculate mean residuals in left and right regions\n",
        "                left_value = np.mean(residuals[left_indices]) if np.sum(left_indices) > 0 else 0\n",
        "                right_value = np.mean(residuals[right_indices]) if np.sum(right_indices) > 0 else 0\n",
        "\n",
        "\n",
        "                # Calculate the residual sum of squares error for this split\n",
        "                error = (\n",
        "                    np.sum((residuals[left_indices] - left_value) ** 2) +\n",
        "                    np.sum((residuals[right_indices] - right_value) ** 2)\n",
        "                )\n",
        "\n",
        "\n",
        "                # Update the stump parameters if this split is better\n",
        "                if error < min_error:\n",
        "                    min_error = error\n",
        "                    self.feature_index = feature_i\n",
        "                    self.threshold = threshold\n",
        "                    self.left_value = left_value\n",
        "                    self.right_value = right_value\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the residuals for each sample in X based on the threshold split.\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        np.array: Predicted residuals\n",
        "        \"\"\"\n",
        "        feature_values = X[:, self.feature_index]\n",
        "        predictions = np.where(feature_values < self.threshold, self.left_value, self.right_value)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GradientBoostingClassifier:\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier that combines weak learners to minimize classification error.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=50, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.stumps = []\n",
        "        self.stump_weights = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Gradient Boosting model on the training data.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "        y (np.array): Target labels (assumed to be -1 and 1 for binary classification)\n",
        "        \"\"\"\n",
        "        # Initialize predictions to zero\n",
        "        y_pred = np.zeros(len(y))\n",
        "\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Calculate residuals\n",
        "            residuals = y - y_pred\n",
        "\n",
        "\n",
        "            # Train a new decision stump on the residuals\n",
        "            stump = DecisionStump()\n",
        "            stump.fit(X, residuals)\n",
        "            stump_predictions = stump.predict(X)\n",
        "\n",
        "\n",
        "            # Calculate stump weight and update predictions\n",
        "            # Note: For a classification task, we multiply by learning_rate to control step size\n",
        "            y_pred += self.learning_rate * stump_predictions\n",
        "\n",
        "\n",
        "            # Save the stump\n",
        "            self.stumps.append(stump)\n",
        "            self.stump_weights.append(self.learning_rate)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels for samples in X.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        X (np.array): Feature matrix\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        np.array: Predicted labels (-1 or 1)\n",
        "        \"\"\"\n",
        "        # Initialize predictions to zero\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "\n",
        "\n",
        "        # Aggregate predictions from each stump\n",
        "        for stump, weight in zip(self.stumps, self.stump_weights):\n",
        "            y_pred += weight * stump.predict(X)\n",
        "\n",
        "\n",
        "        # Return final prediction\n",
        "        return np.sign(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample dataset\n",
        "X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 5]])\n",
        "y = np.array([1, 1, -1, -1, 1])\n",
        "\n",
        "\n",
        "# Convert labels from (0, 1) to (-1, 1) if necessary\n",
        "y = np.where(y == 0, -1, y)\n",
        "\n",
        "\n",
        "# Initialize and train Gradient Boosting\n",
        "model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW25hZAehGoM"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg1FwTwdhGoM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    A simple decision tree classifier used as a base learner in the random forest.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, n_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_features = n_features\n",
        "        self.tree = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Build a decision tree by recursively splitting the data based on the best feature.\n",
        "        \"\"\"\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(self.n_features, X.shape[1])\n",
        "        self.tree = self._grow_tree(X, y)\n",
        "\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively grow the tree by finding the best split.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        num_class_labels = len(np.unique(y))\n",
        "\n",
        "\n",
        "        # Stop conditions\n",
        "        if depth >= self.max_depth or num_class_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return {\"leaf\": True, \"value\": leaf_value}\n",
        "\n",
        "\n",
        "        # Randomly select features to consider for splitting\n",
        "        feature_indices = np.random.choice(n_features, self.n_features, replace=False)\n",
        "\n",
        "\n",
        "        # Find the best split\n",
        "        best_feature, best_threshold = self._best_split(X, y, feature_indices)\n",
        "        if best_feature is None:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return {\"leaf\": True, \"value\": leaf_value}\n",
        "\n",
        "\n",
        "        # Split the dataset\n",
        "        left_indices = X[:, best_feature] < best_threshold\n",
        "        right_indices = ~left_indices\n",
        "        left_subtree = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return {\"leaf\": False, \"feature\": best_feature, \"threshold\": best_threshold, \"left\": left_subtree, \"right\": right_subtree}\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y, feature_indices):\n",
        "        \"\"\"\n",
        "        Find the best feature and threshold to split on based on Gini impurity.\n",
        "        \"\"\"\n",
        "        best_gini = float(\"inf\")\n",
        "        split = {\"feature\": None, \"threshold\": None}\n",
        "\n",
        "\n",
        "        for feature_index in feature_indices:\n",
        "            X_column = X[:, feature_index]\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_indices = X_column < threshold\n",
        "                right_indices = ~left_indices\n",
        "\n",
        "\n",
        "                if np.sum(left_indices) == 0 or np.sum(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                gini = self._gini(y[left_indices], y[right_indices])\n",
        "\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    split[\"feature\"] = feature_index\n",
        "                    split[\"threshold\"] = threshold\n",
        "\n",
        "\n",
        "        return split[\"feature\"], split[\"threshold\"]\n",
        "\n",
        "\n",
        "    def _gini(self, left_labels, right_labels):\n",
        "        \"\"\"\n",
        "        Calculate the Gini impurity of a split.\n",
        "        \"\"\"\n",
        "        def gini_impurity(labels):\n",
        "            classes, counts = np.unique(labels, return_counts=True)\n",
        "            probs = counts / len(labels)\n",
        "            return 1 - np.sum(probs ** 2)\n",
        "\n",
        "\n",
        "        n_left, n_right = len(left_labels), len(right_labels)\n",
        "        n_total = n_left + n_right\n",
        "\n",
        "\n",
        "        gini_left = gini_impurity(left_labels)\n",
        "        gini_right = gini_impurity(right_labels)\n",
        "\n",
        "\n",
        "        return (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"\n",
        "        Find the most common label in a set of labels.\n",
        "        \"\"\"\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class label for each sample in X.\n",
        "        \"\"\"\n",
        "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
        "\n",
        "\n",
        "    def _predict_sample(self, x, tree):\n",
        "        \"\"\"\n",
        "        Recursively traverse the tree to predict the label for a single sample.\n",
        "        \"\"\"\n",
        "        if tree[\"leaf\"]:\n",
        "            return tree[\"value\"]\n",
        "\n",
        "\n",
        "        feature_value = x[tree[\"feature\"]]\n",
        "        if feature_value < tree[\"threshold\"]:\n",
        "            return self._predict_sample(x, tree[\"left\"])\n",
        "        else:\n",
        "            return self._predict_sample(x, tree[\"right\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class RandomForestClassifier:\n",
        "    \"\"\"\n",
        "    Random Forest Classifier that aggregates predictions from multiple decision trees.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, n_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.n_features = n_features\n",
        "        self.trees = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the random forest by training each decision tree on a bootstrap sample.\n",
        "        \"\"\"\n",
        "        self.trees = []\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, n_features=self.n_features)\n",
        "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def _bootstrap_sample(self, X, y):\n",
        "        \"\"\"\n",
        "        Generate a bootstrap sample from the dataset (sampling with replacement).\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "        return X[indices], y[indices]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the class labels by taking a majority vote across all trees.\n",
        "        \"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return self._majority_vote(tree_preds)\n",
        "\n",
        "\n",
        "    def _majority_vote(self, predictions):\n",
        "        \"\"\"\n",
        "        Perform a majority vote to determine the final predicted class.\n",
        "        \"\"\"\n",
        "        return np.array([Counter(preds).most_common(1)[0][0] for preds in predictions.T])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample dataset\n",
        "X = np.array([[1, 2], [2, 3], [3, 1], [4, 3], [5, 5]])\n",
        "y = np.array([0, 0, 1, 1, 0])\n",
        "\n",
        "\n",
        "# Initialize and train the random forest\n",
        "model = RandomForestClassifier(n_estimators=10, max_depth=3)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnjP62WwhGoM"
      },
      "source": [
        "BernoulliNB classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4tvvnZShGoM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class BernoulliNB:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.classes, class_counts = np.unique(y, return_counts=True)\n",
        "        self.class_priors = class_counts / len(y)\n",
        "\n",
        "\n",
        "        self.feature_probs = {}\n",
        "\n",
        "\n",
        "        for cls in self.classes:\n",
        "            X_cls = X[y == cls]\n",
        "\n",
        "            feature_prob = (X_cls.sum(axis=0) + self.alpha) / (X_cls.shape[0] + 2 * self.alpha)\n",
        "\n",
        "            self.feature_probs[cls] = feature_prob\n",
        "\n",
        "\n",
        "    def predict_log_proba(self, X):\n",
        "        log_probs = []\n",
        "\n",
        "\n",
        "        for cls in self.classes:\n",
        "            log_prior = np.log(self.class_priors[np.where(self.classes == cls)][0])\n",
        "\n",
        "            log_likelihood = X * np.log(self.feature_probs[cls]) + (1 - X) * np.log(1 - self.feature_probs[cls])\n",
        "            total_log_prob = log_prior + log_likelihood.sum(axis=1)\n",
        "\n",
        "\n",
        "            log_probs.append(total_log_prob)\n",
        "\n",
        "\n",
        "        return np.array(log_probs).T\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_probs = self.predict_log_proba(X)\n",
        "        return self.classes[np.argmax(log_probs, axis=1)]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sample binary dataset\n",
        "X = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1], [0, 0, 0]])\n",
        "y = np.array([1, 0, 1, 0])\n",
        "\n",
        "\n",
        "model = BernoulliNB(alpha=1.0)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjay51crhGoN"
      },
      "source": [
        "Calibrated Classifier CV:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr5ibUCShGoN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "class CalibratedClassifierCV:\n",
        "    def __init__(self, base_estimator, method='sigmoid', cv=5):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.method = method\n",
        "        self.cv = cv\n",
        "        self.calibrators = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        kf = KFold(n_splits=self.cv)\n",
        "\n",
        "        oof_preds = np.zeros(len(y))\n",
        "        oof_labels = np.zeros(len(y))\n",
        "\n",
        "\n",
        "        for train_idx, calibrate_idx in kf.split(X):\n",
        "            X_train, y_train = X[train_idx], y[train_idx]\n",
        "            X_calibrate, y_calibrate = X[calibrate_idx], y[calibrate_idx]\n",
        "\n",
        "            estimator = self._clone_base_estimator()\n",
        "            estimator.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "            probas = estimator.predict_proba(X_calibrate)[:, 1]\n",
        "\n",
        "\n",
        "            oof_preds[calibrate_idx] = probas\n",
        "            oof_labels[calibrate_idx] = y_calibrate\n",
        "\n",
        "            if self.method == 'sigmoid':\n",
        "                calibrator = LogisticRegression()\n",
        "                calibrator.fit(probas.reshape(-1, 1), y_calibrate)\n",
        "                self.calibrators.append(calibrator)\n",
        "            else:\n",
        "                raise ValueError(\"Currently, only 'sigmoid' (Platt scaling) is implemented.\")\n",
        "\n",
        "\n",
        "        self.base_estimator.fit(X, y)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probas = self.base_estimator.predict_proba(X)[:, 1]\n",
        "        calibrated_probas = np.zeros(len(probas))\n",
        "\n",
        "\n",
        "        for calibrator in self.calibrators:\n",
        "            calibrated_probas += calibrator.predict_proba(probas.reshape(-1, 1))[:, 1]\n",
        "\n",
        "        calibrated_probas /= len(self.calibrators)\n",
        "        return np.vstack([1 - calibrated_probas, calibrated_probas]).T\n",
        "\n",
        "\n",
        "    def _clone_base_estimator(self):\n",
        "        return type(self.base_estimator)(**self.base_estimator.get_params())\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        calibrated_probas = self.predict_proba(X)\n",
        "        return (calibrated_probas[:, 1] >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "\n",
        "base_estimator = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "calibrated_clf = CalibratedClassifierCV(base_estimator, method='sigmoid', cv=5)\n",
        "\n",
        "\n",
        "calibrated_clf.fit(X, y)\n",
        "\n",
        "\n",
        "calibrated_probs = calibrated_clf.predict_proba(X)\n",
        "calibrated_preds = calibrated_clf.predict(X)\n",
        "\n",
        "\n",
        "print(\"Calibrated Probabilities:\", calibrated_probs[:5])\n",
        "print(\"Calibrated Predictions:\", calibrated_preds[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxMcTvkRhGoN"
      },
      "source": [
        "Logistic Regression CV:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P04Fay2ehGoN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "class LogisticRegressionCV:\n",
        "    def __init__(self, Cs=None, cv=5, solver='lbfgs', max_iter=100):\n",
        "        if Cs is None:\n",
        "            Cs = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "        self.Cs = Cs\n",
        "        self.cv = cv\n",
        "        self.solver = solver\n",
        "        self.max_iter = max_iter\n",
        "        self.best_C = None\n",
        "        self.best_model = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        kf = KFold(n_splits=self.cv)\n",
        "        best_score = -np.inf\n",
        "\n",
        "\n",
        "        for C in self.Cs:\n",
        "            scores = []\n",
        "\n",
        "\n",
        "            for train_idx, test_idx in kf.split(X):\n",
        "                X_train, X_test = X[train_idx], X[test_idx]\n",
        "                y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "\n",
        "                model = LogisticRegression(C=C, solver=self.solver, max_iter=self.max_iter)\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "                score = model.score(X_test, y_test)\n",
        "                scores.append(score)\n",
        "\n",
        "\n",
        "            mean_score = np.mean(scores)\n",
        "\n",
        "\n",
        "            if mean_score > best_score:\n",
        "                best_score = mean_score\n",
        "                self.best_C = C\n",
        "                self.best_model = model\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.best_model is None:\n",
        "            raise Exception(\"You must fit the model before predicting.\")\n",
        "        return self.best_model.predict(X)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if self.best_model is None:\n",
        "            raise Exception(\"You must fit the model before predicting.\")\n",
        "        return self.best_model.predict_proba(X)\n",
        "\n",
        "\n",
        "    def get_best_params(self):\n",
        "        return self.best_C\n",
        "\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "\n",
        "logistic_cv = LogisticRegressionCV(Cs=[0.01, 0.1, 1.0, 10.0, 100.0], cv=5)\n",
        "logistic_cv.fit(X, y)\n",
        "\n",
        "\n",
        "predicted_probs = logistic_cv.predict_proba(X)\n",
        "predicted_labels = logistic_cv.predict(X)\n",
        "\n",
        "\n",
        "print(\"Best C:\", logistic_cv.get_best_params())\n",
        "print(\"Predicted Probabilities (first 5):\", predicted_probs[:5])\n",
        "print(\"Predicted Labels (first 5):\", predicted_labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97uokZvwhGoN"
      },
      "source": [
        "Logistic Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQHCtZlXhGoN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, max_iter=1000, tolerance=1e-6):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iter = max_iter\n",
        "        self.tolerance = tolerance\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "\n",
        "        for _ in range(self.max_iter):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "\n",
        "            if np.linalg.norm(dw) < self.tolerance and abs(db) < self.tolerance:\n",
        "                break\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        return (y_predicted >= 0.5).astype(int)\n",
        "\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._sigmoid(linear_model)\n",
        "        return np.vstack([1 - y_predicted, y_predicted]).T\n",
        "\n",
        "\n",
        "    def score(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        return np.mean(predictions == y)\n",
        "\n",
        "\n",
        "# Generate sample data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "\n",
        "logistic_regression = LogisticRegression(learning_rate=0.01, max_iter=1000)\n",
        "logistic_regression.fit(X, y)\n",
        "\n",
        "\n",
        "predicted_probs = logistic_regression.predict_proba(X)\n",
        "predicted_labels = logistic_regression.predict(X)\n",
        "\n",
        "\n",
        "accuracy = logistic_regression.score(X, y)\n",
        "\n",
        "\n",
        "print(\"Predicted Probabilities (first 5):\", predicted_probs[:5])\n",
        "print(\"Predicted Labels (first 5):\", predicted_labels[:5])\n",
        "print(\"Model Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNN-pTthGoN"
      },
      "source": [
        "MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN2XedSehGoN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define activation functions and their derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    logp = -np.log(y_pred[range(n_samples), y_true])\n",
        "    loss = np.sum(logp) / n_samples\n",
        "    return loss\n",
        "\n",
        "\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    n_samples = y_true.shape[0]\n",
        "    grad = y_pred\n",
        "    grad[range(n_samples), y_true] -= 1\n",
        "    grad = grad / n_samples\n",
        "    return grad\n",
        "\n",
        "\n",
        "class MLPClassifier:\n",
        "    \"\"\"\n",
        "    Multilayer Perceptron (MLP) Classifier with a single hidden layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate=0.01, epochs=1000):\n",
        "        # Network architecture\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_outputs = n_outputs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.n_inputs, self.n_hidden) * 0.01\n",
        "        self.bias_hidden = np.zeros((1, self.n_hidden))\n",
        "        self.weights_hidden_output = np.random.randn(self.n_hidden, self.n_outputs) * 0.01\n",
        "        self.bias_output = np.zeros((1, self.n_outputs))\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward propagation through the network.\n",
        "        \"\"\"\n",
        "        # Input to hidden layer\n",
        "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = relu(self.hidden_input)\n",
        "\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output = softmax(self.output_input)\n",
        "        return self.output\n",
        "\n",
        "\n",
        "    def backward(self, X, y):\n",
        "        \"\"\"\n",
        "        Backward propagation to update weights and biases.\n",
        "        \"\"\"\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        # Compute gradient for output layer\n",
        "        output_error = cross_entropy_derivative(y, self.output)\n",
        "        d_weights_hidden_output = np.dot(self.hidden_output.T, output_error)\n",
        "        d_bias_output = np.sum(output_error, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        # Backpropagate error to hidden layer\n",
        "        hidden_error = np.dot(output_error, self.weights_hidden_output.T) * relu_derivative(self.hidden_input)\n",
        "        d_weights_input_hidden = np.dot(X.T, hidden_error)\n",
        "        d_bias_hidden = np.sum(hidden_error, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output -= self.learning_rate * d_weights_hidden_output\n",
        "        self.bias_output -= self.learning_rate * d_bias_output\n",
        "        self.weights_input_hidden -= self.learning_rate * d_weights_input_hidden\n",
        "        self.bias_hidden -= self.learning_rate * d_bias_hidden\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the MLP using forward and backward propagation.\n",
        "        \"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            predictions = self.forward(X)\n",
        "\n",
        "\n",
        "            # Loss calculation\n",
        "            loss = cross_entropy_loss(y, predictions)\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss}\")\n",
        "\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward(X, y)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "        \"\"\"\n",
        "        output_probs = self.forward(X)\n",
        "        return np.argmax(output_probs, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])  # XOR problem\n",
        "\n",
        "\n",
        "# Initialize MLP Classifier with 2 input neurons, 3 hidden neurons, and 2 output neurons\n",
        "mlp = MLPClassifier(n_inputs=2, n_hidden=3, n_outputs=2, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "\n",
        "# Convert labels to one-hot encoding for softmax\n",
        "y_onehot = np.zeros((y.size, 2))\n",
        "y_onehot[np.arange(y.size), y] = 1\n",
        "\n",
        "\n",
        "# Train the model\n",
        "mlp.fit(X, y)\n",
        "\n",
        "\n",
        "# Make predictions\n",
        "predictions = mlp.predict(X)\n",
        "print(\"Predictions:\", predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwoekFxJhGoN"
      },
      "source": [
        "LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTaOYCyihGoN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LinearDiscriminantAnalysis:\n",
        "    \"\"\"\n",
        "    Linear Discriminant Analysis (LDA) Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components=None):\n",
        "        self.n_components = n_components\n",
        "        self.means_ = None\n",
        "        self.scalings_ = None\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit LDA model to training data.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix, shape (n_samples, n_features)\n",
        "        - y: Target labels, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "        class_labels = np.unique(y)\n",
        "\n",
        "        # Calculate the mean vectors for each class\n",
        "        mean_vectors = []\n",
        "        for c in class_labels:\n",
        "            mean_vectors.append(np.mean(X[y == c], axis=0))\n",
        "        overall_mean = np.mean(X, axis=0)\n",
        "\n",
        "\n",
        "        # Initialize within-class and between-class scatter matrices\n",
        "        S_W = np.zeros((n_features, n_features))\n",
        "        S_B = np.zeros((n_features, n_features))\n",
        "\n",
        "\n",
        "        # Compute within-class scatter matrix S_W\n",
        "        for i, c in enumerate(class_labels):\n",
        "            class_scatter = np.cov(X[y == c].T) * (X[y == c].shape[0] - 1)\n",
        "            S_W += class_scatter\n",
        "\n",
        "\n",
        "        # Compute between-class scatter matrix S_B\n",
        "        for i, mean_vec in enumerate(mean_vectors):\n",
        "            n = X[y == class_labels[i]].shape[0]\n",
        "            mean_diff = (mean_vec - overall_mean).reshape(n_features, 1)\n",
        "            S_B += n * (mean_diff).dot(mean_diff.T)\n",
        "\n",
        "\n",
        "        # Solve the generalized eigenvalue problem for S_W^-1 * S_B\n",
        "        eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
        "\n",
        "\n",
        "        # Sort eigenvectors by descending eigenvalues\n",
        "        sorted_indices = np.argsort(eigen_values)[::-1]\n",
        "        eigen_values = eigen_values[sorted_indices]\n",
        "        eigen_vectors = eigen_vectors[:, sorted_indices]\n",
        "\n",
        "\n",
        "        # Select the top n_components eigenvectors\n",
        "        if self.n_components:\n",
        "            eigen_vectors = eigen_vectors[:, :self.n_components]\n",
        "\n",
        "        self.scalings_ = eigen_vectors\n",
        "\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Project data onto the LDA components.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix, shape (n_samples, n_features)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - X_lda: Transformed feature matrix, shape (n_samples, n_components)\n",
        "        \"\"\"\n",
        "        return np.dot(X, self.scalings_)\n",
        "\n",
        "\n",
        "    def fit_transform(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the model and then transform X to LDA components.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix, shape (n_samples, n_features)\n",
        "        - y: Target labels, shape (n_samples,)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - X_lda: Transformed feature matrix, shape (n_samples, n_components)\n",
        "        \"\"\"\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "\n",
        "        Parameters:\n",
        "        - X: Feature matrix, shape (n_samples, n_features)\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels, shape (n_samples,)\n",
        "        \"\"\"\n",
        "        X_lda = self.transform(X)\n",
        "        distances = []\n",
        "        for x in X_lda:\n",
        "            distances.append([np.linalg.norm(x - mean) for mean in self.means_])\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Create a sample dataset\n",
        "X = np.array([[4, 2], [2, 4], [2, 3], [3, 6], [4, 4], [9, 10], [6, 8], [9, 5], [8, 7], [10, 8]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "\n",
        "# Initialize LDA with 1 component\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "\n",
        "# Fit and transform the data\n",
        "X_lda = lda.fit_transform(X, y)\n",
        "print(\"Transformed X (LDA components):\\n\", X_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5IZB-6QhGoO"
      },
      "source": [
        "ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgHEbaqJhGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Single Decision Tree used in Extra Trees Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, max_features=None, max_depth=None, min_samples_split=2):\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Finds a random split for the data by selecting random feature values.\n",
        "        \"\"\"\n",
        "        m, n = X.shape\n",
        "        if self.max_features is None:\n",
        "            self.max_features = n\n",
        "\n",
        "\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_gain = -1\n",
        "        # Select a subset of features to examine\n",
        "        features = np.random.choice(n, self.max_features, replace=False)\n",
        "\n",
        "\n",
        "        for feature in features:\n",
        "            # Randomly choose a threshold within the feature's range\n",
        "            thresholds = np.random.uniform(min(X[:, feature]), max(X[:, feature]), 10)\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, feature] < threshold\n",
        "                right_idx = X[:, feature] >= threshold\n",
        "\n",
        "\n",
        "                # Calculate information gain\n",
        "                if len(y[left_idx]) > 0 and len(y[right_idx]) > 0:\n",
        "                    gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                    if gain > best_gain:\n",
        "                        best_gain, best_feature, best_threshold = gain, feature, threshold\n",
        "\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, parent, left_child, right_child):\n",
        "        \"\"\"\n",
        "        Calculates the information gain of a split.\n",
        "        \"\"\"\n",
        "        weight_left = len(left_child) / len(parent)\n",
        "        weight_right = 1 - weight_left\n",
        "        return self._gini(parent) - (weight_left * self._gini(left_child) + weight_right * self._gini(right_child))\n",
        "\n",
        "\n",
        "    def _gini(self, y):\n",
        "        \"\"\"\n",
        "        Computes the Gini impurity for an array of classes.\n",
        "        \"\"\"\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return 1 - np.sum(ps ** 2)\n",
        "\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively builds the decision tree.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "\n",
        "        # Stopping criteria\n",
        "        if depth >= self.max_depth or num_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return leaf_value\n",
        "\n",
        "\n",
        "        feature, threshold = self._best_split(X, y)\n",
        "        if feature is None:\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "\n",
        "        left_idx = X[:, feature] < threshold\n",
        "        right_idx = X[:, feature] >= threshold\n",
        "        left_subtree = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
        "        return (feature, threshold, left_subtree, right_subtree)\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        \"\"\"\n",
        "        Returns the most common label in an array.\n",
        "        \"\"\"\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "\n",
        "    def _predict(self, inputs, node):\n",
        "        \"\"\"\n",
        "        Predicts a single data point by traversing the tree.\n",
        "        \"\"\"\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "\n",
        "\n",
        "        feature, threshold, left, right = node\n",
        "        if inputs[feature] < threshold:\n",
        "            return self._predict(inputs, left)\n",
        "        else:\n",
        "            return self._predict(inputs, right)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts class labels for X.\n",
        "        \"\"\"\n",
        "        return [self._predict(inputs, self.tree) for inputs in X]\n",
        "\n",
        "\n",
        "class ExtraTreesClassifier:\n",
        "    \"\"\"\n",
        "    Extra Trees Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=10, max_features=None, max_depth=None, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train Extra Trees Classifier\n",
        "        \"\"\"\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_features=self.max_features, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for X\n",
        "        \"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)  # Shape: (n_samples, n_estimators)\n",
        "        y_pred = [Counter(tree_pred).most_common(1)[0][0] for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Define a sample dataset\n",
        "X = np.array([[4, 2], [2, 4], [2, 3], [3, 6], [4, 4], [9, 10], [6, 8], [9, 5], [8, 7], [10, 8]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "\n",
        "# Initialize the Extra Trees Classifier\n",
        "extra_trees = ExtraTreesClassifier(n_estimators=10, max_features=1, max_depth=10)\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "extra_trees.fit(X, y)\n",
        "\n",
        "\n",
        "# Predict the class of a sample\n",
        "y_pred = extra_trees.predict(X)\n",
        "print(\"Predictions:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82tsvfXGhGoO"
      },
      "source": [
        "BaggingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl7Xf3aBhGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Single Decision Tree used in Bagging Classifier\n",
        "    \"\"\"\n",
        "    def __init__(self, max_features=None, max_depth=None, min_samples_split=2):\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.tree = None\n",
        "\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        m, n = X.shape\n",
        "        if self.max_features is None:\n",
        "            self.max_features = n\n",
        "\n",
        "\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_gain = -1\n",
        "        # Select a subset of features to examine\n",
        "        features = np.random.choice(n, self.max_features, replace=False)\n",
        "\n",
        "\n",
        "        for feature in features:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for threshold in thresholds:\n",
        "                left_idx = X[:, feature] < threshold\n",
        "                right_idx = X[:, feature] >= threshold\n",
        "\n",
        "\n",
        "                if len(y[left_idx]) > 0 and len(y[right_idx]) > 0:\n",
        "                    gain = self._information_gain(y, y[left_idx], y[right_idx])\n",
        "                    if gain > best_gain:\n",
        "                        best_gain, best_feature, best_threshold = gain, feature, threshold\n",
        "\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "\n",
        "    def _information_gain(self, parent, left_child, right_child):\n",
        "        weight_left = len(left_child) / len(parent)\n",
        "        weight_right = 1 - weight_left\n",
        "        return self._gini(parent) - (weight_left * self._gini(left_child) + weight_right * self._gini(right_child))\n",
        "\n",
        "\n",
        "    def _gini(self, y):\n",
        "        hist = np.bincount(y)\n",
        "        ps = hist / len(y)\n",
        "        return 1 - np.sum(ps ** 2)\n",
        "\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        n_samples, n_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "\n",
        "        if depth >= self.max_depth or num_labels == 1 or n_samples < self.min_samples_split:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return leaf_value\n",
        "\n",
        "\n",
        "        feature, threshold = self._best_split(X, y)\n",
        "        if feature is None:\n",
        "            return self._most_common_label(y)\n",
        "\n",
        "\n",
        "        left_idx = X[:, feature] < threshold\n",
        "        right_idx = X[:, feature] >= threshold\n",
        "        left_subtree = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
        "        return (feature, threshold, left_subtree, right_subtree)\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        counter = Counter(y)\n",
        "        return counter.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.tree = self._build_tree(X, y)\n",
        "\n",
        "\n",
        "    def _predict(self, inputs, node):\n",
        "        if not isinstance(node, tuple):\n",
        "            return node\n",
        "\n",
        "\n",
        "        feature, threshold, left, right = node\n",
        "        if inputs[feature] < threshold:\n",
        "            return self._predict(inputs, left)\n",
        "        else:\n",
        "            return self._predict(inputs, right)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(inputs, self.tree) for inputs in X]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BaggingClassifier:\n",
        "    \"\"\"\n",
        "    Bagging Classifier with Decision Tree as the base estimator\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=10, max_features=None, max_depth=None, min_samples_split=2):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = max_features\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.trees = []\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the Bagging Classifier\n",
        "        \"\"\"\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTree(max_features=self.max_features, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_sample, y_sample = X[indices], y[indices]\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for X\n",
        "        \"\"\"\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        tree_preds = np.swapaxes(tree_preds, 0, 1)  # Shape: (n_samples, n_estimators)\n",
        "        y_pred = [Counter(tree_pred).most_common(1)[0][0] for tree_pred in tree_preds]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Define a sample dataset\n",
        "X = np.array([[4, 2], [2, 4], [2, 3], [3, 6], [4, 4], [9, 10], [6, 8], [9, 5], [8, 7], [10, 8]])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
        "\n",
        "\n",
        "# Initialize the Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(n_estimators=10, max_features=1, max_depth=10)\n",
        "\n",
        "\n",
        "# Fit the model\n",
        "bagging_clf.fit(X, y)\n",
        "\n",
        "\n",
        "# Predict the class of a sample\n",
        "y_pred = bagging_clf.predict(X)\n",
        "print(\"Predictions:\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L85soDprhGoO"
      },
      "source": [
        "GaussianNB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBFl43KohGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "class GaussianNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.mean = {}\n",
        "        self.var = {}\n",
        "        self.priors = {}\n",
        "\n",
        "\n",
        "        # Calculate mean, variance, and prior probabilities for each class\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.mean[c] = X_c.mean(axis=0)\n",
        "            self.var[c] = X_c.var(axis=0)\n",
        "            self.priors[c] = X_c.shape[0] / X.shape[0]\n",
        "\n",
        "\n",
        "    def _gaussian_density(self, class_idx, x):\n",
        "        mean = self.mean[class_idx]\n",
        "        var = self.var[class_idx]\n",
        "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        denominator = np.sqrt(2 * np.pi * var)\n",
        "        return numerator / denominator\n",
        "\n",
        "\n",
        "    def _classify(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])  # Prior probability\n",
        "            class_conditional = np.sum(np.log(self._gaussian_density(c, x)))\n",
        "            posterior = prior + class_conditional\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._classify(x) for x in X]\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Train and test the model\n",
        "gnb = GaussianNaiveBayes()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8gfSoiNhGoO"
      },
      "source": [
        "MultinomialNB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR9B8oOEhGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "class MultinomialNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.class_feature_count = {}\n",
        "        self.class_counts = {}\n",
        "        self.class_totals = {}\n",
        "        self.alpha = 1  # Laplace smoothing factor\n",
        "\n",
        "\n",
        "        # Calculate counts for each feature per class\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.class_feature_count[c] = np.sum(X_c, axis=0) + self.alpha\n",
        "            self.class_counts[c] = X_c.shape[0]\n",
        "            self.class_totals[c] = self.class_feature_count[c].sum()\n",
        "\n",
        "\n",
        "        self.class_priors = {c: np.log(self.class_counts[c] / X.shape[0]) for c in self.classes}\n",
        "\n",
        "\n",
        "    def _classify(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = self.class_priors[c]\n",
        "            conditional = np.sum(x * np.log(self.class_feature_count[c] / self.class_totals[c]))\n",
        "            posterior = prior + conditional\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._classify(x) for x in X]\n",
        "\n",
        "\n",
        "# Load the 20 Newsgroups dataset and preprocess\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'sci.space'])\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec = vectorizer.fit_transform(newsgroups.data)\n",
        "X, y = X_vec.toarray(), newsgroups.target\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Train and test the model\n",
        "mnb = MultinomialNaiveBayes()\n",
        "mnb.fit(X_train, y_train)\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-1Q0RLhGoO"
      },
      "source": [
        "ComplementNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ2Y7_VkhGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "class ComplementNaiveBayes:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.alpha = 1  # Laplace smoothing factor\n",
        "        self.class_feature_count = {}\n",
        "        self.class_totals = {}\n",
        "\n",
        "\n",
        "        # Calculate counts for each feature complement per class\n",
        "        for c in self.classes:\n",
        "            X_c = X[y != c]  # Use data from all other classes (complement)\n",
        "            self.class_feature_count[c] = np.sum(X_c, axis=0) + self.alpha\n",
        "            self.class_totals[c] = self.class_feature_count[c].sum()\n",
        "\n",
        "\n",
        "        self.class_priors = {c: np.log((X[y != c].shape[0]) / X.shape[0]) for c in self.classes}\n",
        "\n",
        "\n",
        "    def _classify(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = self.class_priors[c]\n",
        "            conditional = np.sum(x * np.log(self.class_feature_count[c] / self.class_totals[c]))\n",
        "            posterior = prior - conditional  # Note: we use the complement, so we subtract\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmin(posteriors)]  # Pick the class with the smallest posterior\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._classify(x) for x in X]\n",
        "\n",
        "\n",
        "# Load the 20 Newsgroups dataset and preprocess\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['alt.atheism', 'sci.space'])\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec = vectorizer.fit_transform(newsgroups.data)\n",
        "X, y = X_vec.toarray(), newsgroups.target\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Train and test the model\n",
        "cnb = ComplementNaiveBayes()\n",
        "cnb.fit(X_train, y_train)\n",
        "y_pred = cnb.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Complement Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAZxhBoUhGoO"
      },
      "source": [
        "Quadratic Discriminant Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8aGyHrQhGoO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "class QuadraticDiscriminantAnalysis:\n",
        "    def fit(self, X, y):\n",
        "        self.classes = np.unique(y)\n",
        "        self.mean = {}\n",
        "        self.cov = {}\n",
        "        self.priors = {}\n",
        "\n",
        "\n",
        "        # Calculate mean, covariance matrix, and prior probabilities for each class\n",
        "        for c in self.classes:\n",
        "            X_c = X[y == c]\n",
        "            self.mean[c] = np.mean(X_c, axis=0)\n",
        "            self.cov[c] = np.cov(X_c, rowvar=False) + 1e-6 * np.eye(X.shape[1])  # Add regularization\n",
        "            self.priors[c] = X_c.shape[0] / X.shape[0]\n",
        "\n",
        "\n",
        "    def _pdf_multivariate(self, x, mean, cov):\n",
        "        size = len(x)\n",
        "        det = np.linalg.det(cov)\n",
        "        norm_const = 1.0 / (np.power((2 * np.pi), size / 2) * np.sqrt(det))\n",
        "        x_mu = x - mean\n",
        "        inv = np.linalg.inv(cov)\n",
        "        result = np.exp(-0.5 * np.dot(np.dot(x_mu.T, inv), x_mu))\n",
        "        return norm_const * result\n",
        "\n",
        "\n",
        "    def _classify(self, x):\n",
        "        posteriors = []\n",
        "        for c in self.classes:\n",
        "            prior = np.log(self.priors[c])\n",
        "            likelihood = np.log(self._pdf_multivariate(x, self.mean[c], self.cov[c]))\n",
        "            posterior = prior + likelihood\n",
        "            posteriors.append(posterior)\n",
        "        return self.classes[np.argmax(posteriors)]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._classify(x) for x in X]\n",
        "\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Train and test the model\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train)\n",
        "y_pred = qda.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Quadratic Discriminant Analysis Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}